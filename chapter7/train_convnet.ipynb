{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import common.layers as layers\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param_1={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 conv_param_2={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "\n",
    "        conv_output_size_1 = (input_dim[1] - conv_param_1['filter_size'] + 2 * conv_param_1['pad']) // conv_param_1['stride'] + 1\n",
    "        pool_output_size_1 = conv_param_1['filter_num'] * (conv_output_size_1 // 2) * (conv_output_size_1 // 2)\n",
    "\n",
    "        conv_output_size_2 = (conv_output_size_1 // 2 - conv_param_2['filter_size'] + 2 * conv_param_2['pad']) // conv_param_2['stride'] + 1\n",
    "        pool_output_size_2 = conv_param_2['filter_num'] * (conv_output_size_2 // 2) * (conv_output_size_2 // 2)\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(conv_param_1['filter_num'], input_dim[0], conv_param_1['filter_size'], conv_param_1['filter_size'])\n",
    "        self.params['b1'] = np.zeros(conv_param_1['filter_num'])\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(conv_param_2['filter_num'], conv_param_1['filter_num'], conv_param_2['filter_size'], conv_param_2['filter_size'])\n",
    "        self.params['b2'] = np.zeros(conv_param_2['filter_num'])\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(pool_output_size_2, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)\n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b4'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = layers.Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad'])\n",
    "        self.layers['Relu1'] = layers.Relu()\n",
    "        self.layers['Pool1'] = layers.Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Conv2'] = layers.Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad'])\n",
    "        self.layers['Relu2'] = layers.Relu()\n",
    "        self.layers['Pool2'] = layers.Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = layers.Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Relu3'] = layers.Relu()\n",
    "        self.layers['Affine2'] = layers.Affine(self.params['W4'], self.params['b4'])\n",
    "\n",
    "        self.last_layer = layers.SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3022151152325603\n",
      "=== epoch:1, train acc:0.15, test acc:0.165 ===\n",
      "train loss:2.302107622990834\n",
      "train loss:2.3017812623408886\n",
      "train loss:2.30120119910658\n",
      "train loss:2.2991989866184444\n",
      "train loss:2.299185391573182\n",
      "train loss:2.300468163183612\n",
      "train loss:2.294832822556624\n",
      "train loss:2.2854324805075836\n",
      "train loss:2.2899497078458055\n",
      "train loss:2.2795715799065044\n",
      "train loss:2.2598632852377687\n",
      "train loss:2.2354939266041636\n",
      "train loss:2.259156102094678\n",
      "train loss:2.250414594946545\n",
      "train loss:2.2135402542830223\n",
      "train loss:2.232769257157855\n",
      "train loss:2.1825717220815886\n",
      "train loss:2.1336944098233017\n",
      "train loss:2.154958637079398\n",
      "train loss:2.091854638391038\n",
      "train loss:2.0502534558450964\n",
      "train loss:2.0042190724128135\n",
      "train loss:1.8878867680786886\n",
      "train loss:1.765723185998919\n",
      "train loss:1.8155329041256305\n",
      "train loss:1.7121366446847701\n",
      "train loss:1.5991175765019068\n",
      "train loss:1.4299711440023914\n",
      "train loss:1.4377024573225992\n",
      "train loss:1.3419615619723007\n",
      "train loss:1.368436509624166\n",
      "train loss:1.1678511444780821\n",
      "train loss:1.1856083971856268\n",
      "train loss:1.050228670852302\n",
      "train loss:1.145212049146479\n",
      "train loss:0.8247618042388699\n",
      "train loss:0.980518659481973\n",
      "train loss:1.0003026224461145\n",
      "train loss:0.8728744216538563\n",
      "train loss:1.139068409651529\n",
      "train loss:0.7344441276119049\n",
      "train loss:0.8915319308022147\n",
      "train loss:0.7881362101873357\n",
      "train loss:0.686893564123433\n",
      "train loss:0.7774824537427737\n",
      "train loss:0.5933806168696375\n",
      "train loss:0.6783552111923785\n",
      "train loss:0.7290057656794993\n",
      "train loss:0.5153916972345753\n",
      "train loss:0.5879026908811432\n",
      "=== epoch:2, train acc:0.744, test acc:0.754 ===\n",
      "train loss:0.6115655969755255\n",
      "train loss:0.6310534034797081\n",
      "train loss:0.5345667105570314\n",
      "train loss:0.73298125806369\n",
      "train loss:0.7159242361479008\n",
      "train loss:0.7246933449284143\n",
      "train loss:0.5487223313426037\n",
      "train loss:0.5652744548229225\n",
      "train loss:0.7718087183149556\n",
      "train loss:0.5618540013860983\n",
      "train loss:0.4250886035112048\n",
      "train loss:0.587766708712256\n",
      "train loss:0.6969098984470423\n",
      "train loss:0.44564613066951075\n",
      "train loss:0.6203924717607641\n",
      "train loss:0.5479795116208411\n",
      "train loss:0.510206220130464\n",
      "train loss:0.47258955105055145\n",
      "train loss:0.6594972221773561\n",
      "train loss:0.38455643807920686\n",
      "train loss:0.3733465533747387\n",
      "train loss:0.41714552082777756\n",
      "train loss:0.5139951849554533\n",
      "train loss:0.640387835326539\n",
      "train loss:0.49789664984565735\n",
      "train loss:0.38801798967342016\n",
      "train loss:0.5450254176322548\n",
      "train loss:0.7654446140526132\n",
      "train loss:0.491793022905383\n",
      "train loss:0.5730278864426122\n",
      "train loss:0.48103711220292816\n",
      "train loss:0.4406471987192176\n",
      "train loss:0.5014073180865755\n",
      "train loss:0.5924643054416083\n",
      "train loss:0.5068116789497454\n",
      "train loss:0.6681431083906467\n",
      "train loss:0.4152581802753494\n",
      "train loss:0.3278515975578319\n",
      "train loss:0.3801109878043045\n",
      "train loss:0.5491149652437065\n",
      "train loss:0.39264495800401295\n",
      "train loss:0.5389255912525518\n",
      "train loss:0.4267560258690753\n",
      "train loss:0.4772429751631861\n",
      "train loss:0.41209011466483025\n",
      "train loss:0.5510758036006681\n",
      "train loss:0.3118848704766163\n",
      "train loss:0.4247690629466536\n",
      "train loss:0.3474557490650828\n",
      "train loss:0.3854669270102321\n",
      "=== epoch:3, train acc:0.842, test acc:0.842 ===\n",
      "train loss:0.3572032477879097\n",
      "train loss:0.6873502511607741\n",
      "train loss:0.4988208028212993\n",
      "train loss:0.4617559175656962\n",
      "train loss:0.32348259238481225\n",
      "train loss:0.3453971964699668\n",
      "train loss:0.3743077918605253\n",
      "train loss:0.3081162969648551\n",
      "train loss:0.3959121805709632\n",
      "train loss:0.40609516354603975\n",
      "train loss:0.40349848308705644\n",
      "train loss:0.29351111985932216\n",
      "train loss:0.5678928826506899\n",
      "train loss:0.3817861488715423\n",
      "train loss:0.3029497631989517\n",
      "train loss:0.285580611410675\n",
      "train loss:0.5755332639116821\n",
      "train loss:0.509207585756105\n",
      "train loss:0.4915876058206792\n",
      "train loss:0.2807485790173032\n",
      "train loss:0.35423919050910563\n",
      "train loss:0.2813476770011187\n",
      "train loss:0.47671717819680026\n",
      "train loss:0.3034722512131716\n",
      "train loss:0.352636813669653\n",
      "train loss:0.3226721738695387\n",
      "train loss:0.269972442040204\n",
      "train loss:0.3625693837647754\n",
      "train loss:0.35272850139550277\n",
      "train loss:0.34775118638010766\n",
      "train loss:0.4106409889748579\n",
      "train loss:0.2734735597258959\n",
      "train loss:0.43734467943010197\n",
      "train loss:0.3730787588987034\n",
      "train loss:0.34152227163152665\n",
      "train loss:0.4241160153663676\n",
      "train loss:0.2859602673552175\n",
      "train loss:0.2510186682868305\n",
      "train loss:0.3435058682398914\n",
      "train loss:0.477250533139872\n",
      "train loss:0.2387650788380145\n",
      "train loss:0.27893371739351475\n",
      "train loss:0.2798280342429082\n",
      "train loss:0.3568179895972549\n",
      "train loss:0.3066619466764744\n",
      "train loss:0.27046440848811226\n",
      "train loss:0.1845473356888742\n",
      "train loss:0.37076011462316855\n",
      "train loss:0.2637844170179031\n",
      "train loss:0.21093368369937845\n",
      "=== epoch:4, train acc:0.885, test acc:0.889 ===\n",
      "train loss:0.17494697640477155\n",
      "train loss:0.20982723012389617\n",
      "train loss:0.27393589143170943\n",
      "train loss:0.4080129215076797\n",
      "train loss:0.29150633765973494\n",
      "train loss:0.24302058724238026\n",
      "train loss:0.3391020686226113\n",
      "train loss:0.31156757789184986\n",
      "train loss:0.4445388554129\n",
      "train loss:0.422099691129594\n",
      "train loss:0.29807387829584564\n",
      "train loss:0.2982395499744522\n",
      "train loss:0.310690058264398\n",
      "train loss:0.2561463811860621\n",
      "train loss:0.3387089269654517\n",
      "train loss:0.22097181534095967\n",
      "train loss:0.3017799368404208\n",
      "train loss:0.40890264193043285\n",
      "train loss:0.14124023270477154\n",
      "train loss:0.31636689184654154\n",
      "train loss:0.24973387134222308\n",
      "train loss:0.3141039384950281\n",
      "train loss:0.18367413678003625\n",
      "train loss:0.1365638752299232\n",
      "train loss:0.36159276206066937\n",
      "train loss:0.2691373510775823\n",
      "train loss:0.24347225440530454\n",
      "train loss:0.2848485604465081\n",
      "train loss:0.38286213102592875\n",
      "train loss:0.3544223532643126\n",
      "train loss:0.15772049978977265\n",
      "train loss:0.24522342655703042\n",
      "train loss:0.3204736838518663\n",
      "train loss:0.24634563528441464\n",
      "train loss:0.2974375255645705\n",
      "train loss:0.4927994180571609\n",
      "train loss:0.1955068808589881\n",
      "train loss:0.14021247128235767\n",
      "train loss:0.2618631652249708\n",
      "train loss:0.38580452252395575\n",
      "train loss:0.21361392432628154\n",
      "train loss:0.2699391182147291\n",
      "train loss:0.2844184530571441\n",
      "train loss:0.30252968262841756\n",
      "train loss:0.2460260478894212\n",
      "train loss:0.20268640707032567\n",
      "train loss:0.29735591741760653\n",
      "train loss:0.2756856989221429\n",
      "train loss:0.1849658123211329\n",
      "train loss:0.20600231441961386\n",
      "=== epoch:5, train acc:0.905, test acc:0.899 ===\n",
      "train loss:0.28614194582418156\n",
      "train loss:0.14229975461892053\n",
      "train loss:0.2430955344485476\n",
      "train loss:0.30247694504723815\n",
      "train loss:0.25074145785588686\n",
      "train loss:0.20459627590980958\n",
      "train loss:0.2092712854563776\n",
      "train loss:0.12024331028329353\n",
      "train loss:0.2638687252561218\n",
      "train loss:0.2656464485991753\n",
      "train loss:0.18754241028497204\n",
      "train loss:0.22018132038968932\n",
      "train loss:0.20870998062920942\n",
      "train loss:0.11762030299488638\n",
      "train loss:0.14268446650396227\n",
      "train loss:0.4281641142905404\n",
      "train loss:0.19182128060855647\n",
      "train loss:0.1741662147631229\n",
      "train loss:0.1739126101308497\n",
      "train loss:0.1403252609651461\n",
      "train loss:0.17247530763818378\n",
      "train loss:0.27482089633810847\n",
      "train loss:0.21212694270048146\n",
      "train loss:0.2915818717520885\n",
      "train loss:0.13890999735961582\n",
      "train loss:0.1988936408724115\n",
      "train loss:0.20599005856351071\n",
      "train loss:0.1901762025700766\n",
      "train loss:0.2577581319882375\n",
      "train loss:0.20870800072930126\n",
      "train loss:0.1919292781827443\n",
      "train loss:0.16915743831219132\n",
      "train loss:0.16634859898308213\n",
      "train loss:0.20862512568325076\n",
      "train loss:0.11211647807382459\n",
      "train loss:0.27110633928897504\n",
      "train loss:0.24523696819961313\n",
      "train loss:0.3313618932655759\n",
      "train loss:0.1502535833151227\n",
      "train loss:0.2097393385810414\n",
      "train loss:0.22188029161152786\n",
      "train loss:0.37928066589036064\n",
      "train loss:0.16792186997467415\n",
      "train loss:0.15215469549995875\n",
      "train loss:0.22721997422965257\n",
      "train loss:0.12344960938836395\n",
      "train loss:0.19176834828393338\n",
      "train loss:0.217946137975483\n",
      "train loss:0.14939842937634415\n",
      "train loss:0.13899909358505685\n",
      "=== epoch:6, train acc:0.929, test acc:0.911 ===\n",
      "train loss:0.198165089915036\n",
      "train loss:0.1955095727791776\n",
      "train loss:0.31535737760122634\n",
      "train loss:0.16732776112841402\n",
      "train loss:0.2489259431979388\n",
      "train loss:0.21496544843997337\n",
      "train loss:0.17383819295986996\n",
      "train loss:0.12278376661706979\n",
      "train loss:0.1689225112669541\n",
      "train loss:0.21988909986536057\n",
      "train loss:0.09210239108168972\n",
      "train loss:0.15873914924394567\n",
      "train loss:0.20312763899592595\n",
      "train loss:0.13496013747289326\n",
      "train loss:0.10311279213271758\n",
      "train loss:0.3128451576708433\n",
      "train loss:0.1765290635447932\n",
      "train loss:0.1630550599455785\n",
      "train loss:0.14306852040879292\n",
      "train loss:0.20208765810789966\n",
      "train loss:0.2907673600319773\n",
      "train loss:0.306096533004544\n",
      "train loss:0.21766987567895996\n",
      "train loss:0.10599313161636724\n",
      "train loss:0.11090972480279734\n",
      "train loss:0.14545980452170287\n",
      "train loss:0.11036561840534617\n",
      "train loss:0.15445447360934658\n",
      "train loss:0.18220864710477272\n",
      "train loss:0.10331435664671122\n",
      "train loss:0.17875790262604574\n",
      "train loss:0.2681923406281428\n",
      "train loss:0.09278138503883251\n",
      "train loss:0.14900941736197865\n",
      "train loss:0.10423017018934512\n",
      "train loss:0.10430326090498511\n",
      "train loss:0.16432422440347286\n",
      "train loss:0.1376716295794834\n",
      "train loss:0.2187262209821986\n",
      "train loss:0.0847445928789218\n",
      "train loss:0.2110554013608487\n",
      "train loss:0.269107932527422\n",
      "train loss:0.11134864968945447\n",
      "train loss:0.14550729707221388\n",
      "train loss:0.11273533597845767\n",
      "train loss:0.13938951473459588\n",
      "train loss:0.14045471202316523\n",
      "train loss:0.2117857656534036\n",
      "train loss:0.12320311306329357\n",
      "train loss:0.07928582788352165\n",
      "=== epoch:7, train acc:0.942, test acc:0.929 ===\n",
      "train loss:0.14243376500473792\n",
      "train loss:0.18581776656815865\n",
      "train loss:0.1322409563301857\n",
      "train loss:0.13793600438519898\n",
      "train loss:0.09913511207535412\n",
      "train loss:0.10076291033417535\n",
      "train loss:0.17882397057808747\n",
      "train loss:0.1145866717656833\n",
      "train loss:0.1352988518977832\n",
      "train loss:0.12562101910876924\n",
      "train loss:0.13379812885323106\n",
      "train loss:0.09565212857357958\n",
      "train loss:0.12358795255299115\n",
      "train loss:0.1867458265244584\n",
      "train loss:0.21686713471917282\n",
      "train loss:0.12998371396855615\n",
      "train loss:0.1349393370284314\n",
      "train loss:0.10644838715753613\n",
      "train loss:0.1220397543051679\n",
      "train loss:0.07643334654609532\n",
      "train loss:0.07492810103032868\n",
      "train loss:0.17605145486840995\n",
      "train loss:0.15014314160624787\n",
      "train loss:0.05705638639806236\n",
      "train loss:0.13193761859841205\n",
      "train loss:0.13490375333714652\n",
      "train loss:0.12199900099530422\n",
      "train loss:0.1380273581385738\n",
      "train loss:0.1804928940881385\n",
      "train loss:0.18074725211340037\n",
      "train loss:0.13885785561290567\n",
      "train loss:0.1211282690811906\n",
      "train loss:0.1040009590666269\n",
      "train loss:0.07264906765912665\n",
      "train loss:0.15505118667926848\n",
      "train loss:0.09357984065487786\n",
      "train loss:0.12447218226861075\n",
      "train loss:0.1567047585071442\n",
      "train loss:0.19677875270461853\n",
      "train loss:0.19875138071979725\n",
      "train loss:0.055005512092651546\n",
      "train loss:0.13904543851848994\n",
      "train loss:0.10721118199790855\n",
      "train loss:0.07420119432373419\n",
      "train loss:0.13855416847199345\n",
      "train loss:0.0936139491204425\n",
      "train loss:0.1666801775924835\n",
      "train loss:0.23891578950056902\n",
      "train loss:0.2036752872154826\n",
      "train loss:0.16606188215782824\n",
      "=== epoch:8, train acc:0.949, test acc:0.936 ===\n",
      "train loss:0.24454087721891032\n",
      "train loss:0.08598502150084174\n",
      "train loss:0.0724978269396541\n",
      "train loss:0.08225656657707034\n",
      "train loss:0.050242303885455045\n",
      "train loss:0.1279357192620415\n",
      "train loss:0.0806995002212228\n",
      "train loss:0.2221824106967971\n",
      "train loss:0.22300280378129564\n",
      "train loss:0.10291002718945444\n",
      "train loss:0.11368999145724595\n",
      "train loss:0.18344856663828651\n",
      "train loss:0.1700452247268017\n",
      "train loss:0.2616862709418834\n",
      "train loss:0.20063048868499792\n",
      "train loss:0.1028454469009883\n",
      "train loss:0.1578985701273697\n",
      "train loss:0.12018037958460337\n",
      "train loss:0.15146307523703104\n",
      "train loss:0.1250241239861306\n",
      "train loss:0.12842797805967632\n",
      "train loss:0.17208847091384827\n",
      "train loss:0.04461446320842552\n",
      "train loss:0.07956137918768134\n",
      "train loss:0.125154316968656\n",
      "train loss:0.1946286700029638\n",
      "train loss:0.07729416814187261\n",
      "train loss:0.1560483913413073\n",
      "train loss:0.20170179669800478\n",
      "train loss:0.031758125015442784\n",
      "train loss:0.08770462462208699\n",
      "train loss:0.25527416730014857\n",
      "train loss:0.09807797502334538\n",
      "train loss:0.15426033190876628\n",
      "train loss:0.08708757784599898\n",
      "train loss:0.07486407659896144\n",
      "train loss:0.08250599941952785\n",
      "train loss:0.10173600763647064\n",
      "train loss:0.08457878866649614\n",
      "train loss:0.04849113807874703\n",
      "train loss:0.2211947285688349\n",
      "train loss:0.23667426305385258\n",
      "train loss:0.19502762273447996\n",
      "train loss:0.130707341456821\n",
      "train loss:0.0978946304552589\n",
      "train loss:0.1873146352212424\n",
      "train loss:0.12685210063025237\n",
      "train loss:0.12083517712975621\n",
      "train loss:0.1010220649888246\n",
      "train loss:0.048113260748683315\n",
      "=== epoch:9, train acc:0.951, test acc:0.94 ===\n",
      "train loss:0.13864164405256763\n",
      "train loss:0.21808342665366598\n",
      "train loss:0.09252876603552135\n",
      "train loss:0.04920155907234447\n",
      "train loss:0.16299948278035942\n",
      "train loss:0.0845838442257511\n",
      "train loss:0.16693046885512633\n",
      "train loss:0.15003372315314123\n",
      "train loss:0.15353671995901302\n",
      "train loss:0.09404884423333501\n",
      "train loss:0.1947323509701709\n",
      "train loss:0.17940692409658213\n",
      "train loss:0.2146138782567691\n",
      "train loss:0.17139500277155836\n",
      "train loss:0.10366496036847356\n",
      "train loss:0.24028742406529047\n",
      "train loss:0.08866293898681735\n",
      "train loss:0.09231884872667127\n",
      "train loss:0.0662427632153565\n",
      "train loss:0.18535582268280104\n",
      "train loss:0.09323631421605404\n",
      "train loss:0.07201226325279514\n",
      "train loss:0.09722171588676934\n",
      "train loss:0.0878779840329424\n",
      "train loss:0.08123652023935742\n",
      "train loss:0.14222960179172647\n",
      "train loss:0.07861235063651106\n",
      "train loss:0.04950053980959385\n",
      "train loss:0.10295482136787809\n",
      "train loss:0.052102306475739775\n",
      "train loss:0.11171864275055907\n",
      "train loss:0.11080533750000172\n",
      "train loss:0.1366259905405553\n",
      "train loss:0.08152629034276535\n",
      "train loss:0.10567646409772177\n",
      "train loss:0.23128367534319452\n",
      "train loss:0.09547386150842\n",
      "train loss:0.13943695901319494\n",
      "train loss:0.16138200514766396\n",
      "train loss:0.11706977899220894\n",
      "train loss:0.10325423579736892\n",
      "train loss:0.085300855405755\n",
      "train loss:0.051609734352698416\n",
      "train loss:0.03674471997097725\n",
      "train loss:0.08848607344797253\n",
      "train loss:0.06909177396108143\n",
      "train loss:0.08745368452630607\n",
      "train loss:0.07860504454054813\n",
      "train loss:0.05813400689927655\n",
      "train loss:0.07411736323233085\n",
      "=== epoch:10, train acc:0.961, test acc:0.93 ===\n",
      "train loss:0.07966801518289886\n",
      "train loss:0.05815979008445726\n",
      "train loss:0.053492292461110674\n",
      "train loss:0.0753994287975228\n",
      "train loss:0.0944594871775549\n",
      "train loss:0.18755291861411996\n",
      "train loss:0.06003835570094352\n",
      "train loss:0.022180890692579763\n",
      "train loss:0.07547430183998492\n",
      "train loss:0.0620453173753483\n",
      "train loss:0.0728733736752658\n",
      "train loss:0.1323287190728614\n",
      "train loss:0.10540578958904664\n",
      "train loss:0.13570417499882279\n",
      "train loss:0.06001265357163006\n",
      "train loss:0.09105558514763926\n",
      "train loss:0.09885394025940494\n",
      "train loss:0.1250897984261862\n",
      "train loss:0.06915614484160136\n",
      "train loss:0.1518899078000593\n",
      "train loss:0.13999802904744788\n",
      "train loss:0.16172523586986412\n",
      "train loss:0.045879890527739525\n",
      "train loss:0.15771424751248359\n",
      "train loss:0.10612711260071862\n",
      "train loss:0.12388381641026706\n",
      "train loss:0.22571024014903357\n",
      "train loss:0.12405855567658261\n",
      "train loss:0.06364678341530046\n",
      "train loss:0.05784068546383124\n",
      "train loss:0.043003216845509326\n",
      "train loss:0.07283029171071725\n",
      "train loss:0.10559622987836184\n",
      "train loss:0.03305995194072653\n",
      "train loss:0.022730611182092585\n",
      "train loss:0.08508090317441333\n",
      "train loss:0.1072180119446122\n",
      "train loss:0.16308137285403215\n",
      "train loss:0.04727965638787755\n",
      "train loss:0.09245719200084002\n",
      "train loss:0.18980699735918996\n",
      "train loss:0.08077405573091513\n",
      "train loss:0.10553117491576167\n",
      "train loss:0.11112192975692135\n",
      "train loss:0.04630925967909562\n",
      "train loss:0.13654438020299714\n",
      "train loss:0.07829658410636574\n",
      "train loss:0.09185192599498264\n",
      "train loss:0.060605083738829064\n",
      "train loss:0.09479221179838701\n",
      "=== epoch:11, train acc:0.973, test acc:0.956 ===\n",
      "train loss:0.07799602025222732\n",
      "train loss:0.07589294182564313\n",
      "train loss:0.049700904852963496\n",
      "train loss:0.04090349674926025\n",
      "train loss:0.06295295082723047\n",
      "train loss:0.15410262998501786\n",
      "train loss:0.06954779329252365\n",
      "train loss:0.03366916736920429\n",
      "train loss:0.11080499452405596\n",
      "train loss:0.07205833682854579\n",
      "train loss:0.1457164188742728\n",
      "train loss:0.09414883188394801\n",
      "train loss:0.131075240913867\n",
      "train loss:0.09670042828408673\n",
      "train loss:0.0383332786599565\n",
      "train loss:0.03502929645598037\n",
      "train loss:0.10462729060443214\n",
      "train loss:0.12942782294728464\n",
      "train loss:0.08906573017532084\n",
      "train loss:0.09869107766592028\n",
      "train loss:0.06855876067618424\n",
      "train loss:0.08516450929914988\n",
      "train loss:0.07787632567382807\n",
      "train loss:0.05267545615921909\n",
      "train loss:0.22429338182015182\n",
      "train loss:0.12047669787628015\n",
      "train loss:0.07691179109491347\n",
      "train loss:0.0734940991598901\n",
      "train loss:0.05436507266072224\n",
      "train loss:0.04327649194763624\n",
      "train loss:0.11996359060693712\n",
      "train loss:0.045837512458576396\n",
      "train loss:0.07602022939908418\n",
      "train loss:0.07189424256704652\n",
      "train loss:0.17995824341784894\n",
      "train loss:0.07390478545017451\n",
      "train loss:0.021713558727622057\n",
      "train loss:0.13706691012828534\n",
      "train loss:0.03778942299252939\n",
      "train loss:0.1226780817728105\n",
      "train loss:0.08333827853226104\n",
      "train loss:0.09898540308407915\n",
      "train loss:0.08837258549295127\n",
      "train loss:0.04748246516377661\n",
      "train loss:0.08424532713198428\n",
      "train loss:0.09084461912320327\n",
      "train loss:0.05840955120437705\n",
      "train loss:0.07833769361457157\n",
      "train loss:0.08070459074605746\n",
      "train loss:0.09827723976132276\n",
      "=== epoch:12, train acc:0.981, test acc:0.962 ===\n",
      "train loss:0.1520365296261093\n",
      "train loss:0.060291310154887805\n",
      "train loss:0.10371067891045196\n",
      "train loss:0.04691937547912224\n",
      "train loss:0.04440700451428083\n",
      "train loss:0.05645485111150479\n",
      "train loss:0.03402942835644106\n",
      "train loss:0.10890091925713802\n",
      "train loss:0.057069097235180576\n",
      "train loss:0.10176849699529336\n",
      "train loss:0.06496167567395002\n",
      "train loss:0.10142954861813386\n",
      "train loss:0.059208628259221185\n",
      "train loss:0.04704894037323922\n",
      "train loss:0.10328862085997943\n",
      "train loss:0.02944289792654053\n",
      "train loss:0.0341559032003371\n",
      "train loss:0.05052810991399192\n",
      "train loss:0.044381702706168716\n",
      "train loss:0.04462817836143301\n",
      "train loss:0.047520671926028865\n",
      "train loss:0.08285489438717741\n",
      "train loss:0.040957088626845904\n",
      "train loss:0.08322995051222859\n",
      "train loss:0.04374569651093536\n",
      "train loss:0.13519040281292777\n",
      "train loss:0.02256338275547535\n",
      "train loss:0.061371402501563165\n",
      "train loss:0.07363340816860363\n",
      "train loss:0.02843599484830856\n",
      "train loss:0.07400655316146348\n",
      "train loss:0.04689445876457353\n",
      "train loss:0.08962894090353038\n",
      "train loss:0.06918707495183134\n",
      "train loss:0.029399330048271142\n",
      "train loss:0.043900633461518554\n",
      "train loss:0.039779748348518315\n",
      "train loss:0.04218867443883199\n",
      "train loss:0.0297275673152242\n",
      "train loss:0.04214465847269419\n",
      "train loss:0.042503932722423345\n",
      "train loss:0.03650714410076238\n",
      "train loss:0.05054756518082097\n",
      "train loss:0.10244658812514845\n",
      "train loss:0.03572743746874414\n",
      "train loss:0.051356790188343666\n",
      "train loss:0.02181758813485106\n",
      "train loss:0.10359242295762237\n",
      "train loss:0.06007801216146814\n",
      "train loss:0.036575224102329225\n",
      "=== epoch:13, train acc:0.977, test acc:0.956 ===\n",
      "train loss:0.058773334719034585\n",
      "train loss:0.029835878198528242\n",
      "train loss:0.08318703745950028\n",
      "train loss:0.014962716765106243\n",
      "train loss:0.026453961533722627\n",
      "train loss:0.036339039160262876\n",
      "train loss:0.1319675931330693\n",
      "train loss:0.04975816449756935\n",
      "train loss:0.07483379267367184\n",
      "train loss:0.03934791193817074\n",
      "train loss:0.029860176717996026\n",
      "train loss:0.03689612654107571\n",
      "train loss:0.15307521110746794\n",
      "train loss:0.03496490901225291\n",
      "train loss:0.0186809349547674\n",
      "train loss:0.0469257810758128\n",
      "train loss:0.06419711779832148\n",
      "train loss:0.03865388108716617\n",
      "train loss:0.04661364162691378\n",
      "train loss:0.10968876841993934\n",
      "train loss:0.05252578635511017\n",
      "train loss:0.027569217355711596\n",
      "train loss:0.07538289696477918\n",
      "train loss:0.05150325791236363\n",
      "train loss:0.06358928426045064\n",
      "train loss:0.03638508034089704\n",
      "train loss:0.049341840142195934\n",
      "train loss:0.046439071778472396\n",
      "train loss:0.08491069555673318\n",
      "train loss:0.036568873386645155\n",
      "train loss:0.029355561588001216\n",
      "train loss:0.08287781974942873\n",
      "train loss:0.06218027634169585\n",
      "train loss:0.08824924116025099\n",
      "train loss:0.03187823431365413\n",
      "train loss:0.12249947615413737\n",
      "train loss:0.016337210392293775\n",
      "train loss:0.05414060024049876\n",
      "train loss:0.028033481537776052\n",
      "train loss:0.12986806326405806\n",
      "train loss:0.030364243448979172\n",
      "train loss:0.1428359559512599\n",
      "train loss:0.03152553529033842\n",
      "train loss:0.015850464534987726\n",
      "train loss:0.045662473777438047\n",
      "train loss:0.02137475726385835\n",
      "train loss:0.10303330356654611\n",
      "train loss:0.053730273539023356\n",
      "train loss:0.021619008730862612\n",
      "train loss:0.03165274813996072\n",
      "=== epoch:14, train acc:0.975, test acc:0.962 ===\n",
      "train loss:0.04643946558195368\n",
      "train loss:0.07254669129140048\n",
      "train loss:0.08084065083664747\n",
      "train loss:0.027557485780390592\n",
      "train loss:0.04764417793658472\n",
      "train loss:0.06551951764031813\n",
      "train loss:0.0349723703561266\n",
      "train loss:0.036095733164361585\n",
      "train loss:0.09510875273031495\n",
      "train loss:0.09122234021077726\n",
      "train loss:0.08779883346196611\n",
      "train loss:0.033406444669060904\n",
      "train loss:0.10967668113015773\n",
      "train loss:0.021259838628738002\n",
      "train loss:0.06171075985141318\n",
      "train loss:0.046305412943632225\n",
      "train loss:0.03010857415702294\n",
      "train loss:0.030410907461962188\n",
      "train loss:0.15727143462359883\n",
      "train loss:0.046889323291268464\n",
      "train loss:0.021284918408737852\n",
      "train loss:0.014941171251523437\n",
      "train loss:0.20880708272059298\n",
      "train loss:0.038744515785782974\n",
      "train loss:0.02549771942562116\n",
      "train loss:0.02691018105459742\n",
      "train loss:0.04773039473359697\n",
      "train loss:0.05531655445524983\n",
      "train loss:0.02545790241067839\n",
      "train loss:0.08564514009967691\n",
      "train loss:0.05857651212816275\n",
      "train loss:0.11728685359968197\n",
      "train loss:0.015554971014956136\n",
      "train loss:0.13552294382808422\n",
      "train loss:0.046440536915299285\n",
      "train loss:0.0905812094921841\n",
      "train loss:0.017484383029501102\n",
      "train loss:0.02299695214449552\n",
      "train loss:0.0422414904452503\n",
      "train loss:0.03849815869351655\n",
      "train loss:0.02277265449704707\n",
      "train loss:0.0444750281310299\n",
      "train loss:0.07246915865356571\n",
      "train loss:0.014149623550644766\n",
      "train loss:0.05903253454086252\n",
      "train loss:0.10686639724655914\n",
      "train loss:0.039203234339728846\n",
      "train loss:0.07385121283287155\n",
      "train loss:0.0427983163705499\n",
      "train loss:0.06240547820682628\n",
      "=== epoch:15, train acc:0.985, test acc:0.962 ===\n",
      "train loss:0.02284902030050968\n",
      "train loss:0.04050721024718121\n",
      "train loss:0.047794621891481225\n",
      "train loss:0.05094483510520391\n",
      "train loss:0.028130377319887843\n",
      "train loss:0.03637627285725026\n",
      "train loss:0.02622787941584976\n",
      "train loss:0.020360069812805137\n",
      "train loss:0.028144020136553843\n",
      "train loss:0.01378270695264332\n",
      "train loss:0.024037467668471292\n",
      "train loss:0.06509163100163842\n",
      "train loss:0.035350683991971825\n",
      "train loss:0.0709044263036627\n",
      "train loss:0.1014398126531787\n",
      "train loss:0.0412640014773748\n",
      "train loss:0.1086118415051637\n",
      "train loss:0.04967499290156148\n",
      "train loss:0.030697035528659513\n",
      "train loss:0.03206841017726756\n",
      "train loss:0.023868791445424642\n",
      "train loss:0.03848183930652234\n",
      "train loss:0.10794276803256894\n",
      "train loss:0.010909817788072944\n",
      "train loss:0.10189189459388627\n",
      "train loss:0.036559184255923985\n",
      "train loss:0.03544697176987194\n",
      "train loss:0.03175211168156301\n",
      "train loss:0.05783454666837196\n",
      "train loss:0.016048353914604498\n",
      "train loss:0.04179818089440941\n",
      "train loss:0.007555542809138675\n",
      "train loss:0.04224357306550141\n",
      "train loss:0.05499035263576342\n",
      "train loss:0.04723149371616188\n",
      "train loss:0.02046760116954122\n",
      "train loss:0.07904823782772459\n",
      "train loss:0.038962360171942496\n",
      "train loss:0.01566786608344861\n",
      "train loss:0.033125285031998235\n",
      "train loss:0.03020057981678944\n",
      "train loss:0.025560392654682906\n",
      "train loss:0.034962311563429736\n",
      "train loss:0.12251442026802482\n",
      "train loss:0.11808770640712707\n",
      "train loss:0.0161272745437222\n",
      "train loss:0.03278467067359689\n",
      "train loss:0.04436442657780607\n",
      "train loss:0.062496013058605096\n",
      "train loss:0.057292515060835235\n",
      "=== epoch:16, train acc:0.979, test acc:0.954 ===\n",
      "train loss:0.017563269927648607\n",
      "train loss:0.01606841911907904\n",
      "train loss:0.03726175311679205\n",
      "train loss:0.15408408070809212\n",
      "train loss:0.030710594749314483\n",
      "train loss:0.05456746880177441\n",
      "train loss:0.03943779967680241\n",
      "train loss:0.006263908327977855\n",
      "train loss:0.09358913381930027\n",
      "train loss:0.027914062344252807\n",
      "train loss:0.030879455902444272\n",
      "train loss:0.012172015124463375\n",
      "train loss:0.025535956342367997\n",
      "train loss:0.05021153748832874\n",
      "train loss:0.03967864881616173\n",
      "train loss:0.07653161803962\n",
      "train loss:0.005533713953028395\n",
      "train loss:0.041967596174144736\n",
      "train loss:0.11847917085377166\n",
      "train loss:0.033902522060074436\n",
      "train loss:0.0614826243205662\n",
      "train loss:0.04765690398077599\n",
      "train loss:0.0636293588124018\n",
      "train loss:0.029195298949404272\n",
      "train loss:0.03751101066951392\n",
      "train loss:0.03243712379437313\n",
      "train loss:0.04272747681730207\n",
      "train loss:0.06625656372347791\n",
      "train loss:0.04349560905590662\n",
      "train loss:0.01574332580315047\n",
      "train loss:0.021793694971894307\n",
      "train loss:0.020966248070978306\n",
      "train loss:0.019566141223348234\n",
      "train loss:0.025731896418121108\n",
      "train loss:0.029528984603675826\n",
      "train loss:0.04056007665144725\n",
      "train loss:0.02153317233793674\n",
      "train loss:0.07133275666218217\n",
      "train loss:0.09626708775889428\n",
      "train loss:0.05983588479295117\n",
      "train loss:0.03463629399357834\n",
      "train loss:0.04031241781250248\n",
      "train loss:0.053295294176339054\n",
      "train loss:0.05607912086339936\n",
      "train loss:0.016980083359834953\n",
      "train loss:0.03317073750419624\n",
      "train loss:0.032787136611097355\n",
      "train loss:0.03987552236823308\n",
      "train loss:0.05772014635752445\n",
      "train loss:0.039607057051635364\n",
      "=== epoch:17, train acc:0.982, test acc:0.962 ===\n",
      "train loss:0.05567172780023385\n",
      "train loss:0.015445429068178576\n",
      "train loss:0.036461531208973524\n",
      "train loss:0.0077562950158944245\n",
      "train loss:0.03372301448316957\n",
      "train loss:0.030013997313353805\n",
      "train loss:0.05206128628060779\n",
      "train loss:0.04425928720249595\n",
      "train loss:0.013706723388534436\n",
      "train loss:0.027233533820248117\n",
      "train loss:0.05507683156936321\n",
      "train loss:0.01920037674073154\n",
      "train loss:0.04437129271075791\n",
      "train loss:0.04928061794128706\n",
      "train loss:0.021391350093892574\n",
      "train loss:0.08705062087793589\n",
      "train loss:0.04845871156755524\n",
      "train loss:0.09044267323998965\n",
      "train loss:0.016262515273829846\n",
      "train loss:0.03091818112775\n",
      "train loss:0.04755655827871677\n",
      "train loss:0.07787385858819504\n",
      "train loss:0.0441305915206957\n",
      "train loss:0.015052998311705385\n",
      "train loss:0.01359547421388764\n",
      "train loss:0.031892135957454035\n",
      "train loss:0.023091991447183085\n",
      "train loss:0.0384478398959861\n",
      "train loss:0.029006107037789254\n",
      "train loss:0.03624660720425567\n",
      "train loss:0.028870724355049178\n",
      "train loss:0.03075615304048334\n",
      "train loss:0.030988175949180196\n",
      "train loss:0.022390143645504054\n",
      "train loss:0.023965719139061658\n",
      "train loss:0.03123458940793806\n",
      "train loss:0.0312161447867578\n",
      "train loss:0.03332366944293169\n",
      "train loss:0.03488240353016339\n",
      "train loss:0.025645146103044712\n",
      "train loss:0.06305842358964013\n",
      "train loss:0.01908614646095575\n",
      "train loss:0.01606800232863315\n",
      "train loss:0.00865050408209212\n",
      "train loss:0.021247734553699375\n",
      "train loss:0.01824460252958711\n",
      "train loss:0.024182738511240653\n",
      "train loss:0.013778196822975059\n",
      "train loss:0.021432200684865142\n",
      "train loss:0.0675173976085548\n",
      "=== epoch:18, train acc:0.99, test acc:0.966 ===\n",
      "train loss:0.027604432721787805\n",
      "train loss:0.022748460483282405\n",
      "train loss:0.01038631019605642\n",
      "train loss:0.018116659559446354\n",
      "train loss:0.01741668489155145\n",
      "train loss:0.01890151405738111\n",
      "train loss:0.007606416945167021\n",
      "train loss:0.01397687521504008\n",
      "train loss:0.02734631257238816\n",
      "train loss:0.016857451999119203\n",
      "train loss:0.009277549671005414\n",
      "train loss:0.027414880385721704\n",
      "train loss:0.010426528764167836\n",
      "train loss:0.004152366124373069\n",
      "train loss:0.011991265210164016\n",
      "train loss:0.07748902727285643\n",
      "train loss:0.045770076182469026\n",
      "train loss:0.022782025998254133\n",
      "train loss:0.021757527429427933\n",
      "train loss:0.026695647835517398\n",
      "train loss:0.019738696002453846\n",
      "train loss:0.013508293852039922\n",
      "train loss:0.032000058124566515\n",
      "train loss:0.07267728069687042\n",
      "train loss:0.012734873896879797\n",
      "train loss:0.012548200033824468\n",
      "train loss:0.026312824446485795\n",
      "train loss:0.006747207324846488\n",
      "train loss:0.021288189412190985\n",
      "train loss:0.013430918307225412\n",
      "train loss:0.033276421789684936\n",
      "train loss:0.020414421622897437\n",
      "train loss:0.02392850467444783\n",
      "train loss:0.0030680883488589886\n",
      "train loss:0.017958068184842982\n",
      "train loss:0.007882287695185276\n",
      "train loss:0.017758105403176735\n",
      "train loss:0.04362109910363169\n",
      "train loss:0.07490208028987959\n",
      "train loss:0.01737205344472881\n",
      "train loss:0.00819597682637926\n",
      "train loss:0.013833152322077675\n",
      "train loss:0.019486747517044677\n",
      "train loss:0.016678130752192745\n",
      "train loss:0.013623179691818296\n",
      "train loss:0.07590584684737188\n",
      "train loss:0.04038913620820551\n",
      "train loss:0.019655640259257437\n",
      "train loss:0.020034932823408288\n",
      "train loss:0.020174090727572293\n",
      "=== epoch:19, train acc:0.993, test acc:0.966 ===\n",
      "train loss:0.04430482259797568\n",
      "train loss:0.11165225734429247\n",
      "train loss:0.010459910634089839\n",
      "train loss:0.03543476409830776\n",
      "train loss:0.020126889357358815\n",
      "train loss:0.07655566602484146\n",
      "train loss:0.06436053113087879\n",
      "train loss:0.010736370913456566\n",
      "train loss:0.04992447192332765\n",
      "train loss:0.021070369667405397\n",
      "train loss:0.02115114257848304\n",
      "train loss:0.0426409616344066\n",
      "train loss:0.009509836625592594\n",
      "train loss:0.03505164449802935\n",
      "train loss:0.035693679881457135\n",
      "train loss:0.02590579693159296\n",
      "train loss:0.030527169867425853\n",
      "train loss:0.017543076230422647\n",
      "train loss:0.02072050705818225\n",
      "train loss:0.04494340918859199\n",
      "train loss:0.028082366912592895\n",
      "train loss:0.013205062474164113\n",
      "train loss:0.037189510074924825\n",
      "train loss:0.021782266267999208\n",
      "train loss:0.016619273532252798\n",
      "train loss:0.022222532318186366\n",
      "train loss:0.022938223575769308\n",
      "train loss:0.009173247218539073\n",
      "train loss:0.03962877623385332\n",
      "train loss:0.014155200222236925\n",
      "train loss:0.03075542532190421\n",
      "train loss:0.007950688518589918\n",
      "train loss:0.029526900248708618\n",
      "train loss:0.011691724216345815\n",
      "train loss:0.011389394235705598\n",
      "train loss:0.016659739823630185\n",
      "train loss:0.030665153444670273\n",
      "train loss:0.05369029130599562\n",
      "train loss:0.006875619444393094\n",
      "train loss:0.05162890680823283\n",
      "train loss:0.019943543089468826\n",
      "train loss:0.016547686621259546\n",
      "train loss:0.016669374217443234\n",
      "train loss:0.03128779915587707\n",
      "train loss:0.025540700652968498\n",
      "train loss:0.012084502259394246\n",
      "train loss:0.06425682648022525\n",
      "train loss:0.02315321620015546\n",
      "train loss:0.021270238984211442\n",
      "train loss:0.01528906206731918\n",
      "=== epoch:20, train acc:0.993, test acc:0.964 ===\n",
      "train loss:0.04862783794178603\n",
      "train loss:0.020501831287605986\n",
      "train loss:0.01887096687771494\n",
      "train loss:0.009599682810775868\n",
      "train loss:0.025655372846658833\n",
      "train loss:0.03381162643973044\n",
      "train loss:0.015588089459779195\n",
      "train loss:0.02487985612036852\n",
      "train loss:0.02668851675852007\n",
      "train loss:0.02466676951341338\n",
      "train loss:0.03492647254292937\n",
      "train loss:0.03665555623192378\n",
      "train loss:0.011877737138577362\n",
      "train loss:0.02455972313282064\n",
      "train loss:0.06760468545898154\n",
      "train loss:0.05310671409285751\n",
      "train loss:0.03578231688260676\n",
      "train loss:0.005399713039167518\n",
      "train loss:0.020313678948136092\n",
      "train loss:0.007601287698240685\n",
      "train loss:0.018977884204200227\n",
      "train loss:0.055936100513976544\n",
      "train loss:0.027627396176697697\n",
      "train loss:0.017498593422128415\n",
      "train loss:0.010854244191586648\n",
      "train loss:0.01264587992047037\n",
      "train loss:0.0030358208447378876\n",
      "train loss:0.032012896241117764\n",
      "train loss:0.016911586369854622\n",
      "train loss:0.024615332288434554\n",
      "train loss:0.016404933355807606\n",
      "train loss:0.032485986861694384\n",
      "train loss:0.011610556885181613\n",
      "train loss:0.009835677988971591\n",
      "train loss:0.043061449416251424\n",
      "train loss:0.01816355229759743\n",
      "train loss:0.024015531666106423\n",
      "train loss:0.030670975537321753\n",
      "train loss:0.013489267329184668\n",
      "train loss:0.018736926530692928\n",
      "train loss:0.010637502629311497\n",
      "train loss:0.0124514854029364\n",
      "train loss:0.012610928470481649\n",
      "train loss:0.005257480172787832\n",
      "train loss:0.04402299282328193\n",
      "train loss:0.003916225049460071\n",
      "train loss:0.012073416209716132\n",
      "train loss:0.010843741692522071\n",
      "train loss:0.011764832888538272\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.96\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 200\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28), \n",
    "                        conv_param_1={'filter_num': 32, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        conv_param_2={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "network.save_params(\"params_self.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
