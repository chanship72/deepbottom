{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import common.layers as layers\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param_1={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 conv_param_2={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "\n",
    "        conv_output_size_1 = (input_dim[1] - conv_param_1['filter_size'] + 2 * conv_param_1['pad']) // conv_param_1['stride'] + 1\n",
    "        pool_output_size_1 = conv_param_1['filter_num'] * (conv_output_size_1 // 2) * (conv_output_size_1 // 2)\n",
    "\n",
    "        conv_output_size_2 = (conv_output_size_1 // 2 - conv_param_2['filter_size'] + 2 * conv_param_2['pad']) // conv_param_2['stride'] + 1\n",
    "        pool_output_size_2 = conv_param_2['filter_num'] * (conv_output_size_2 // 2) * (conv_output_size_2 // 2)\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(conv_param_1['filter_num'], input_dim[0], conv_param_1['filter_size'], conv_param_1['filter_size'])\n",
    "        self.params['b1'] = np.zeros(conv_param_1['filter_num'])\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(conv_param_2['filter_num'], conv_param_1['filter_num'], conv_param_2['filter_size'], conv_param_2['filter_size'])\n",
    "        self.params['b2'] = np.zeros(conv_param_2['filter_num'])\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(pool_output_size_2, hidden_size)\n",
    "        self.params['b3'] = np.zeros(hidden_size)\n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b4'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = layers.Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad'])\n",
    "        self.layers['Relu1'] = layers.Relu()\n",
    "        self.layers['Pool1'] = layers.Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Conv2'] = layers.Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad'])\n",
    "        self.layers['Relu2'] = layers.Relu()\n",
    "        self.layers['Pool2'] = layers.Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = layers.Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Relu3'] = layers.Relu()\n",
    "        self.layers['Affine2'] = layers.Affine(self.params['W4'], self.params['b4'])\n",
    "\n",
    "        self.last_layer = layers.SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Conv2'].dW, self.layers['Conv2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3021911051702633\n",
      "=== epoch:1, train acc:0.228, test acc:0.245 ===\n",
      "train loss:2.3019521292917786\n",
      "train loss:2.3022265543519356\n",
      "train loss:2.301902099128298\n",
      "train loss:2.3000378060437368\n",
      "train loss:2.300790672121423\n",
      "train loss:2.298558820111007\n",
      "train loss:2.301012335067886\n",
      "train loss:2.297034880213648\n",
      "train loss:2.2954597165454946\n",
      "train loss:2.2931058664234096\n",
      "train loss:2.282503905061649\n",
      "train loss:2.2815631014863937\n",
      "train loss:2.2722787825313113\n",
      "train loss:2.2629826598225957\n",
      "train loss:2.252274685006236\n",
      "train loss:2.2327464954557854\n",
      "train loss:2.210287311081804\n",
      "train loss:2.157879993824109\n",
      "train loss:2.1273457738578223\n",
      "train loss:2.107846559046373\n",
      "train loss:2.001450350514772\n",
      "train loss:1.9474979260715406\n",
      "train loss:1.9614692324242469\n",
      "train loss:1.8378861861243174\n",
      "train loss:1.8039725269697557\n",
      "train loss:1.7439084496920811\n",
      "train loss:1.6627866719816238\n",
      "train loss:1.4533084304328208\n",
      "train loss:1.5470882352176756\n",
      "train loss:1.4006669184623803\n",
      "train loss:1.3558160245184905\n",
      "train loss:1.174726133082207\n",
      "train loss:1.117285503302957\n",
      "train loss:1.0962347729332576\n",
      "train loss:1.114376582395232\n",
      "train loss:1.0066179806649431\n",
      "train loss:0.9486278829216138\n",
      "train loss:0.8955229603927889\n",
      "train loss:0.8967317945131856\n",
      "train loss:0.9922907736292045\n",
      "train loss:0.8819539925330812\n",
      "train loss:0.80389580059936\n",
      "train loss:0.9377754486582256\n",
      "train loss:0.6467163282215158\n",
      "train loss:0.698095288573444\n",
      "train loss:0.7560349074487982\n",
      "train loss:0.7022722456024775\n",
      "train loss:0.6450022537198155\n",
      "train loss:0.7577188776559751\n",
      "train loss:0.638045228408848\n",
      "=== epoch:2, train acc:0.756, test acc:0.693 ===\n",
      "train loss:0.7451449408192112\n",
      "train loss:0.685232556235048\n",
      "train loss:0.7155163723819455\n",
      "train loss:0.9957920751257661\n",
      "train loss:0.7263915622885264\n",
      "train loss:0.5727445268289645\n",
      "train loss:0.7156589574282518\n",
      "train loss:0.7095607529062172\n",
      "train loss:0.492826637738142\n",
      "train loss:0.6468140589720881\n",
      "train loss:0.6226318722065108\n",
      "train loss:0.8013459675092808\n",
      "train loss:0.6665011003255905\n",
      "train loss:0.8480848843104445\n",
      "train loss:0.4939660882438646\n",
      "train loss:0.611098801186881\n",
      "train loss:0.6287305320989746\n",
      "train loss:0.6154337986785896\n",
      "train loss:0.4796452247178921\n",
      "train loss:0.8833846845970663\n",
      "train loss:0.5297810391852277\n",
      "train loss:0.6673649034819705\n",
      "train loss:0.773807664815489\n",
      "train loss:0.6971028675487925\n",
      "train loss:0.5526055115703388\n",
      "train loss:0.6713976450263205\n",
      "train loss:0.5535945016086511\n",
      "train loss:0.47609181685815705\n",
      "train loss:0.4964848956935888\n",
      "train loss:0.46774484150239926\n",
      "train loss:0.42218980405681583\n",
      "train loss:0.6166781006881139\n",
      "train loss:0.4740314693578033\n",
      "train loss:0.3848879795689175\n",
      "train loss:0.5843327034372078\n",
      "train loss:0.5687929378427998\n",
      "train loss:0.44786131759974573\n",
      "train loss:0.6610501979038296\n",
      "train loss:0.5036293823096177\n",
      "train loss:0.5498552446652304\n",
      "train loss:0.4776840101001973\n",
      "train loss:0.3359964189584903\n",
      "train loss:0.4795581469274503\n",
      "train loss:0.3763326772991373\n",
      "train loss:0.42386177695187427\n",
      "train loss:0.38991216419756547\n",
      "train loss:0.3342231737489496\n",
      "train loss:0.4241070720419877\n",
      "train loss:0.2888762822003279\n",
      "train loss:0.5487408105779664\n",
      "=== epoch:3, train acc:0.837, test acc:0.805 ===\n",
      "train loss:0.4322638144377038\n",
      "train loss:0.2861112514227196\n",
      "train loss:0.47378080010503004\n",
      "train loss:0.3295498543381052\n",
      "train loss:0.41823818584237105\n",
      "train loss:0.45856030248910384\n",
      "train loss:0.46689414284478703\n",
      "train loss:0.37903400873951854\n",
      "train loss:0.34844657200922663\n",
      "train loss:0.35250487790005386\n",
      "train loss:0.4272410338664412\n",
      "train loss:0.508769992026973\n",
      "train loss:0.4692079151001891\n",
      "train loss:0.3887590922433957\n",
      "train loss:0.4823590310416322\n",
      "train loss:0.36784484592743943\n",
      "train loss:0.3798031680722974\n",
      "train loss:0.3971012608602715\n",
      "train loss:0.32059195714936367\n",
      "train loss:0.3207593596444038\n",
      "train loss:0.3316881967259665\n",
      "train loss:0.4136871665353298\n",
      "train loss:0.38003051930235493\n",
      "train loss:0.38246868710784354\n",
      "train loss:0.4025775388974835\n",
      "train loss:0.47391658260485614\n",
      "train loss:0.2719686733353914\n",
      "train loss:0.5102189990001493\n",
      "train loss:0.3196277050179224\n",
      "train loss:0.39792490824041626\n",
      "train loss:0.3763365079373546\n",
      "train loss:0.43360523776790677\n",
      "train loss:0.41055516196181313\n",
      "train loss:0.2962479570937389\n",
      "train loss:0.40326419430946053\n",
      "train loss:0.4290401459873147\n",
      "train loss:0.32128323850653423\n",
      "train loss:0.558405843269019\n",
      "train loss:0.5137172398561481\n",
      "train loss:0.42369860786043295\n",
      "train loss:0.36557087454038084\n",
      "train loss:0.45583008789608515\n",
      "train loss:0.38601469453036896\n",
      "train loss:0.2900288824192718\n",
      "train loss:0.35328106481096844\n",
      "train loss:0.5188405941638295\n",
      "train loss:0.37455484677871503\n",
      "train loss:0.22008784584607763\n",
      "train loss:0.22839169917135588\n",
      "train loss:0.30768771702679626\n",
      "=== epoch:4, train acc:0.886, test acc:0.873 ===\n",
      "train loss:0.3080586631405374\n",
      "train loss:0.3514645853772468\n",
      "train loss:0.21805661088344594\n",
      "train loss:0.31582538646770336\n",
      "train loss:0.34265069370256435\n",
      "train loss:0.29834858387849056\n",
      "train loss:0.20889700061191555\n",
      "train loss:0.2856090117197166\n",
      "train loss:0.2979942629700155\n",
      "train loss:0.21528404576079224\n",
      "train loss:0.23387320134992964\n",
      "train loss:0.2726889587850873\n",
      "train loss:0.4360920186153332\n",
      "train loss:0.24327067884782827\n",
      "train loss:0.2640757287191961\n",
      "train loss:0.3096034129389122\n",
      "train loss:0.30587687888640164\n",
      "train loss:0.2591953583983653\n",
      "train loss:0.13509668749614237\n",
      "train loss:0.29306557949006273\n",
      "train loss:0.30248394416405544\n",
      "train loss:0.2961857153822837\n",
      "train loss:0.29668931854259195\n",
      "train loss:0.28296833490361223\n",
      "train loss:0.20736779661610008\n",
      "train loss:0.2614637461667696\n",
      "train loss:0.22691647849882168\n",
      "train loss:0.2721770103104468\n",
      "train loss:0.27074048189038574\n",
      "train loss:0.14441435441458303\n",
      "train loss:0.33929569679305066\n",
      "train loss:0.26047228541331785\n",
      "train loss:0.17259583834744646\n",
      "train loss:0.13821855283334195\n",
      "train loss:0.26362920592278516\n",
      "train loss:0.27477489735178073\n",
      "train loss:0.3660866275463634\n",
      "train loss:0.4073722862677199\n",
      "train loss:0.19736249786190943\n",
      "train loss:0.3258530987634775\n",
      "train loss:0.15835708729022777\n",
      "train loss:0.3686908366061331\n",
      "train loss:0.27446225953581505\n",
      "train loss:0.14005758401828955\n",
      "train loss:0.2353810681205066\n",
      "train loss:0.2588741781292734\n",
      "train loss:0.3190488413593119\n",
      "train loss:0.2536557633226269\n",
      "train loss:0.2946287984104121\n",
      "train loss:0.44524881632086133\n",
      "=== epoch:5, train acc:0.897, test acc:0.898 ===\n",
      "train loss:0.22406889218711096\n",
      "train loss:0.4705437871042348\n",
      "train loss:0.13182838389762735\n",
      "train loss:0.3363176973093377\n",
      "train loss:0.3115562041184601\n",
      "train loss:0.2499687403070552\n",
      "train loss:0.25104155139745904\n",
      "train loss:0.13790413414214966\n",
      "train loss:0.18020616470216033\n",
      "train loss:0.26782195043486295\n",
      "train loss:0.32347677588613755\n",
      "train loss:0.30011857986605195\n",
      "train loss:0.23931621832502029\n",
      "train loss:0.3109255225629664\n",
      "train loss:0.28085372943133174\n",
      "train loss:0.19141157256978023\n",
      "train loss:0.3023930621811226\n",
      "train loss:0.2111154146713822\n",
      "train loss:0.2105915486625215\n",
      "train loss:0.2022478363909096\n",
      "train loss:0.3871980023245704\n",
      "train loss:0.3437794324319278\n",
      "train loss:0.4098147283644637\n",
      "train loss:0.2177414013466087\n",
      "train loss:0.3243365182340165\n",
      "train loss:0.23718574035298953\n",
      "train loss:0.2504692396687813\n",
      "train loss:0.1842478088292248\n",
      "train loss:0.2080162251011965\n",
      "train loss:0.11420837454776377\n",
      "train loss:0.15244853000068395\n",
      "train loss:0.30400298794632724\n",
      "train loss:0.17808552268995975\n",
      "train loss:0.2717154993370367\n",
      "train loss:0.1338682906810628\n",
      "train loss:0.16009717029874831\n",
      "train loss:0.2169124550173639\n",
      "train loss:0.2966134932700082\n",
      "train loss:0.18905891752392667\n",
      "train loss:0.2839006492812825\n",
      "train loss:0.15801458750462943\n",
      "train loss:0.21016360000998469\n",
      "train loss:0.1710149929460897\n",
      "train loss:0.27246754652953076\n",
      "train loss:0.13434494848286305\n",
      "train loss:0.21768212031597972\n",
      "train loss:0.1754478018893209\n",
      "train loss:0.10835554116560234\n",
      "train loss:0.21150456367526171\n",
      "train loss:0.13194012883464518\n",
      "=== epoch:6, train acc:0.929, test acc:0.913 ===\n",
      "train loss:0.09368105203882514\n",
      "train loss:0.0655427161193773\n",
      "train loss:0.16812781933442456\n",
      "train loss:0.2706964230041197\n",
      "train loss:0.2477244734920684\n",
      "train loss:0.18016211245278235\n",
      "train loss:0.12273433377219764\n",
      "train loss:0.08067567348734561\n",
      "train loss:0.19218942527148106\n",
      "train loss:0.2606897143625262\n",
      "train loss:0.11922302008511053\n",
      "train loss:0.18111240156732478\n",
      "train loss:0.1880540906604243\n",
      "train loss:0.2967601041335108\n",
      "train loss:0.2217837299179433\n",
      "train loss:0.1262390515805697\n",
      "train loss:0.12927625240487126\n",
      "train loss:0.13858457411619535\n",
      "train loss:0.22441867141311744\n",
      "train loss:0.22812118977085455\n",
      "train loss:0.10113470888536627\n",
      "train loss:0.18879684707398628\n",
      "train loss:0.2818898038257308\n",
      "train loss:0.07483435180246467\n",
      "train loss:0.1554395637985521\n",
      "train loss:0.17416903511409565\n",
      "train loss:0.15374984625837393\n",
      "train loss:0.14675949132937607\n",
      "train loss:0.33365194266577075\n",
      "train loss:0.17080837973609392\n",
      "train loss:0.15383947725631822\n",
      "train loss:0.13302172961626518\n",
      "train loss:0.12753566005983324\n",
      "train loss:0.16020891879412882\n",
      "train loss:0.16190102899557762\n",
      "train loss:0.2186915477284935\n",
      "train loss:0.19159010962072437\n",
      "train loss:0.13120316698491352\n",
      "train loss:0.1625595424784873\n",
      "train loss:0.1443008249324809\n",
      "train loss:0.16098482403941947\n",
      "train loss:0.11395485244384503\n",
      "train loss:0.20029810476044063\n",
      "train loss:0.12503538259878913\n",
      "train loss:0.1316650494428794\n",
      "train loss:0.17268493075155242\n",
      "train loss:0.18734084429059958\n",
      "train loss:0.22505474048856045\n",
      "train loss:0.16226549649610333\n",
      "train loss:0.1637217519519357\n",
      "=== epoch:7, train acc:0.943, test acc:0.925 ===\n",
      "train loss:0.1666153187176384\n",
      "train loss:0.12391757829465694\n",
      "train loss:0.15696267144578663\n",
      "train loss:0.15002932823443257\n",
      "train loss:0.2039905774520122\n",
      "train loss:0.24784064822931612\n",
      "train loss:0.22959199197992708\n",
      "train loss:0.17211704055287577\n",
      "train loss:0.0987339856570732\n",
      "train loss:0.15011865773982808\n",
      "train loss:0.19122395861220437\n",
      "train loss:0.22400094374615812\n",
      "train loss:0.1703201247774649\n",
      "train loss:0.10119820662332314\n",
      "train loss:0.2060068895519252\n",
      "train loss:0.1281002368616934\n",
      "train loss:0.22867583819546022\n",
      "train loss:0.13791788989552908\n",
      "train loss:0.17914448074983383\n",
      "train loss:0.20085178702844694\n",
      "train loss:0.09245061478120578\n",
      "train loss:0.15294750088470122\n",
      "train loss:0.20555242436758434\n",
      "train loss:0.10183490035613174\n",
      "train loss:0.08300154448169808\n",
      "train loss:0.1862839454964141\n",
      "train loss:0.10676722832370852\n",
      "train loss:0.1433758326515535\n",
      "train loss:0.09677507439972347\n",
      "train loss:0.20636122966547826\n",
      "train loss:0.12163730298996496\n",
      "train loss:0.1537232653046706\n",
      "train loss:0.19416834214029122\n",
      "train loss:0.08977305629486967\n",
      "train loss:0.18621621095955693\n",
      "train loss:0.08216210879483009\n",
      "train loss:0.10312368406392591\n",
      "train loss:0.17479289830667782\n",
      "train loss:0.22258429071851304\n",
      "train loss:0.048338499922038966\n",
      "train loss:0.20116493713014805\n",
      "train loss:0.1754248802138597\n",
      "train loss:0.1764548598489833\n",
      "train loss:0.14806363905283862\n",
      "train loss:0.08643932464578655\n",
      "train loss:0.17048694181557017\n",
      "train loss:0.1373680921913255\n",
      "train loss:0.10477687819119222\n",
      "train loss:0.08974579187679675\n",
      "train loss:0.18974579383716086\n",
      "=== epoch:8, train acc:0.954, test acc:0.946 ===\n",
      "train loss:0.07817486764703281\n",
      "train loss:0.1289370356039939\n",
      "train loss:0.1196487308596486\n",
      "train loss:0.2674851160403279\n",
      "train loss:0.10301419791604954\n",
      "train loss:0.1574822994499388\n",
      "train loss:0.13678409715534823\n",
      "train loss:0.1955270075758593\n",
      "train loss:0.06483089368032549\n",
      "train loss:0.1487089031741243\n",
      "train loss:0.17418490371924367\n",
      "train loss:0.14220887283737998\n",
      "train loss:0.08691439190436943\n",
      "train loss:0.08404821521936146\n",
      "train loss:0.04168643011974013\n",
      "train loss:0.128735186978345\n",
      "train loss:0.14605598367224792\n",
      "train loss:0.09746754350268429\n",
      "train loss:0.14698021502552425\n",
      "train loss:0.15672948534381956\n",
      "train loss:0.20747997439680593\n",
      "train loss:0.12331693861881571\n",
      "train loss:0.08636855153248847\n",
      "train loss:0.09462089203175714\n",
      "train loss:0.09428487153598449\n",
      "train loss:0.13627479791579844\n",
      "train loss:0.1314498825343323\n",
      "train loss:0.0903798237021312\n",
      "train loss:0.09219470508820006\n",
      "train loss:0.13943947047766897\n",
      "train loss:0.06520423066893778\n",
      "train loss:0.06891562384145353\n",
      "train loss:0.09195230911806548\n",
      "train loss:0.08166885320192206\n",
      "train loss:0.08745818339930635\n",
      "train loss:0.1424573094876084\n",
      "train loss:0.08145829179315299\n",
      "train loss:0.07291427836490448\n",
      "train loss:0.06611982390759862\n",
      "train loss:0.1530580744433881\n",
      "train loss:0.17067721100915775\n",
      "train loss:0.055238519805075645\n",
      "train loss:0.12277619092976985\n",
      "train loss:0.10271138255785273\n",
      "train loss:0.13866589994794462\n",
      "train loss:0.16392719914456022\n",
      "train loss:0.13112611395836227\n",
      "train loss:0.0672856427944628\n",
      "train loss:0.03382905202209756\n",
      "train loss:0.22817820755331691\n",
      "=== epoch:9, train acc:0.959, test acc:0.955 ===\n",
      "train loss:0.039823827108979394\n",
      "train loss:0.16101539021454678\n",
      "train loss:0.18384637767263903\n",
      "train loss:0.09985081736063563\n",
      "train loss:0.12300829543647344\n",
      "train loss:0.16419816933961953\n",
      "train loss:0.10901151519631277\n",
      "train loss:0.06988002808810412\n",
      "train loss:0.05618244637302436\n",
      "train loss:0.08422545510077704\n",
      "train loss:0.05844240634287765\n",
      "train loss:0.050507126434654256\n",
      "train loss:0.051288522842875196\n",
      "train loss:0.22567068614921287\n",
      "train loss:0.1197454492863579\n",
      "train loss:0.22526582332158987\n",
      "train loss:0.06132690077532596\n",
      "train loss:0.14122396635000148\n",
      "train loss:0.08711329148545252\n",
      "train loss:0.12953762010059466\n",
      "train loss:0.09817805505423494\n",
      "train loss:0.0668952271501714\n",
      "train loss:0.20202189297004344\n",
      "train loss:0.06446125087931252\n",
      "train loss:0.11262125452520377\n",
      "train loss:0.06636780218412115\n",
      "train loss:0.11330264209904108\n",
      "train loss:0.17152012155448268\n",
      "train loss:0.0511981243574903\n",
      "train loss:0.12504461307177384\n",
      "train loss:0.1853850002956008\n",
      "train loss:0.10648380214559397\n",
      "train loss:0.10280289658501257\n",
      "train loss:0.13583186154883084\n",
      "train loss:0.07255669885107317\n",
      "train loss:0.20924359900505496\n",
      "train loss:0.12839254311092183\n",
      "train loss:0.18220054880777745\n",
      "train loss:0.12234267863472802\n",
      "train loss:0.047574949249526585\n",
      "train loss:0.05371117601410053\n",
      "train loss:0.07932472210918239\n",
      "train loss:0.2583096288389868\n",
      "train loss:0.09683567084265958\n",
      "train loss:0.11361101763522928\n",
      "train loss:0.11131753179967\n",
      "train loss:0.05787621091107033\n",
      "train loss:0.07423401764934041\n",
      "train loss:0.07665071929074815\n",
      "train loss:0.07593430833033182\n",
      "=== epoch:10, train acc:0.962, test acc:0.948 ===\n",
      "train loss:0.05197772394016101\n",
      "train loss:0.045346131960043855\n",
      "train loss:0.11999884950568357\n",
      "train loss:0.07791083871482887\n",
      "train loss:0.04783737695171398\n",
      "train loss:0.05517497554933639\n",
      "train loss:0.11221564608294952\n",
      "train loss:0.052680212917246144\n",
      "train loss:0.0737030216741503\n",
      "train loss:0.08308584676209904\n",
      "train loss:0.08943638520589335\n",
      "train loss:0.05702494183666885\n",
      "train loss:0.09519898919623698\n",
      "train loss:0.05055542481278251\n",
      "train loss:0.13272584974503446\n",
      "train loss:0.062098541792206204\n",
      "train loss:0.1479823144267569\n",
      "train loss:0.07306927256867402\n",
      "train loss:0.1594843892022431\n",
      "train loss:0.10147880503871272\n",
      "train loss:0.13510093312238086\n",
      "train loss:0.044295807672774065\n",
      "train loss:0.12683880672982206\n",
      "train loss:0.08579352093912451\n",
      "train loss:0.12708666165134902\n",
      "train loss:0.0970712220284891\n",
      "train loss:0.04493103280493742\n",
      "train loss:0.057793886304757326\n",
      "train loss:0.08895472462249622\n",
      "train loss:0.10557216226205864\n",
      "train loss:0.07803198241508359\n",
      "train loss:0.1649756857147835\n",
      "train loss:0.13669339444785034\n",
      "train loss:0.16757991906878203\n",
      "train loss:0.10210527944002515\n",
      "train loss:0.08452611822010177\n",
      "train loss:0.06363260145018103\n",
      "train loss:0.08334075241579678\n",
      "train loss:0.12252469825495003\n",
      "train loss:0.07685865117813821\n",
      "train loss:0.05020915074632224\n",
      "train loss:0.03375247123741021\n",
      "train loss:0.06654327114249142\n",
      "train loss:0.032884310175386934\n",
      "train loss:0.03323854586565167\n",
      "train loss:0.14044674835093593\n",
      "train loss:0.07159062893351686\n",
      "train loss:0.1443158341617215\n",
      "train loss:0.09365684005690653\n",
      "train loss:0.047685982551843156\n",
      "=== epoch:11, train acc:0.962, test acc:0.944 ===\n",
      "train loss:0.17804717166656014\n",
      "train loss:0.11676529929665042\n",
      "train loss:0.05357208332492279\n",
      "train loss:0.058342787787158866\n",
      "train loss:0.03469205523829108\n",
      "train loss:0.07811103305365476\n",
      "train loss:0.11781345595530855\n",
      "train loss:0.10138516389550649\n",
      "train loss:0.08645893671960067\n",
      "train loss:0.09550614012826472\n",
      "train loss:0.18132416873255242\n",
      "train loss:0.08369497141272433\n",
      "train loss:0.06937740533165244\n",
      "train loss:0.08320431296031021\n",
      "train loss:0.11199186209528716\n",
      "train loss:0.10426629652733073\n",
      "train loss:0.06138572179886775\n",
      "train loss:0.046066760103958264\n",
      "train loss:0.10068682571528981\n",
      "train loss:0.10295756364439965\n",
      "train loss:0.053499499192894505\n",
      "train loss:0.052912681701810724\n",
      "train loss:0.07549810350722186\n",
      "train loss:0.11834316701996292\n",
      "train loss:0.06360209856475117\n",
      "train loss:0.08925656856474605\n",
      "train loss:0.06464360149300875\n",
      "train loss:0.04708216580820456\n",
      "train loss:0.05871965025314597\n",
      "train loss:0.17098326557087107\n",
      "train loss:0.05310947499055464\n",
      "train loss:0.11406428361905635\n",
      "train loss:0.1326389021902581\n",
      "train loss:0.11693685448984395\n",
      "train loss:0.07867334976351692\n",
      "train loss:0.04091228925357127\n",
      "train loss:0.0982111419001742\n",
      "train loss:0.05883005936583533\n",
      "train loss:0.034654010355482774\n",
      "train loss:0.16167176161822283\n",
      "train loss:0.06717937179407263\n",
      "train loss:0.0624389420125203\n",
      "train loss:0.06685844928963677\n",
      "train loss:0.11410433032482688\n",
      "train loss:0.07519302681921192\n",
      "train loss:0.1550356604592488\n",
      "train loss:0.08160804078850932\n",
      "train loss:0.13685687539860203\n",
      "train loss:0.05553446876181917\n",
      "train loss:0.03153049636925505\n",
      "=== epoch:12, train acc:0.974, test acc:0.959 ===\n",
      "train loss:0.07578676018033645\n",
      "train loss:0.05562258167894711\n",
      "train loss:0.03560605428483379\n",
      "train loss:0.08792996131427992\n",
      "train loss:0.08962587340020707\n",
      "train loss:0.05060429041440986\n",
      "train loss:0.10774095739981031\n",
      "train loss:0.15706966348349347\n",
      "train loss:0.07632317489753504\n",
      "train loss:0.08348948242278434\n",
      "train loss:0.044615753462239505\n",
      "train loss:0.058863501249169836\n",
      "train loss:0.11134826512190416\n",
      "train loss:0.08008700517749727\n",
      "train loss:0.09677586403999068\n",
      "train loss:0.030495152213516015\n",
      "train loss:0.03841776652155594\n",
      "train loss:0.023870410182953133\n",
      "train loss:0.08665078970831493\n",
      "train loss:0.028656496366507224\n",
      "train loss:0.03486536586830515\n",
      "train loss:0.04906271188899308\n",
      "train loss:0.1508949412101975\n",
      "train loss:0.05376685500016188\n",
      "train loss:0.13096261309798418\n",
      "train loss:0.042626870306415876\n",
      "train loss:0.07358800293586928\n",
      "train loss:0.04448568662723604\n",
      "train loss:0.07187523628022195\n",
      "train loss:0.12192801256114663\n",
      "train loss:0.07213187659788968\n",
      "train loss:0.07474530125364047\n",
      "train loss:0.08713773011845709\n",
      "train loss:0.1900802419282314\n",
      "train loss:0.10044865964205524\n",
      "train loss:0.09184204354932503\n",
      "train loss:0.06627246357775063\n",
      "train loss:0.08286848806872092\n",
      "train loss:0.08436272284603515\n",
      "train loss:0.036371949169770865\n",
      "train loss:0.04199375905998782\n",
      "train loss:0.03984729414159421\n",
      "train loss:0.0803347892689967\n",
      "train loss:0.15153144901500867\n",
      "train loss:0.02714145075536998\n",
      "train loss:0.03331654645026285\n",
      "train loss:0.11826551467966614\n",
      "train loss:0.1686417692990311\n",
      "train loss:0.025902277060990287\n",
      "train loss:0.11996032597440844\n",
      "=== epoch:13, train acc:0.975, test acc:0.949 ===\n",
      "train loss:0.016279939634832857\n",
      "train loss:0.047816318471204114\n",
      "train loss:0.034191657813236194\n",
      "train loss:0.030898327753319858\n",
      "train loss:0.04902485472863996\n",
      "train loss:0.06555636700657526\n",
      "train loss:0.024708965366965967\n",
      "train loss:0.1412584911992324\n",
      "train loss:0.05837606044719917\n",
      "train loss:0.049726003458159765\n",
      "train loss:0.10917756780103684\n",
      "train loss:0.13651839022157886\n",
      "train loss:0.11377411657609793\n",
      "train loss:0.061592947449678956\n",
      "train loss:0.029997004778074942\n",
      "train loss:0.2065591958153703\n",
      "train loss:0.05362166593528011\n",
      "train loss:0.07186975924229254\n",
      "train loss:0.09887924495718246\n",
      "train loss:0.04534202063976133\n",
      "train loss:0.04133630798141086\n",
      "train loss:0.04829484703986048\n",
      "train loss:0.030653880478843574\n",
      "train loss:0.09263764799968183\n",
      "train loss:0.041900848870193776\n",
      "train loss:0.06020845144973915\n",
      "train loss:0.0793180061796658\n",
      "train loss:0.07567062035235378\n",
      "train loss:0.0838033185845264\n",
      "train loss:0.030334349137368637\n",
      "train loss:0.05964815523948062\n",
      "train loss:0.11952893863173385\n",
      "train loss:0.06350657118182429\n",
      "train loss:0.028473318593348226\n",
      "train loss:0.028425408563709006\n",
      "train loss:0.020069109747592444\n",
      "train loss:0.05750168396816185\n",
      "train loss:0.03719936899116729\n",
      "train loss:0.05402117269776689\n",
      "train loss:0.04515832287000077\n",
      "train loss:0.07829447897963043\n",
      "train loss:0.13416579080372615\n",
      "train loss:0.039860109225016406\n",
      "train loss:0.08726205328337326\n",
      "train loss:0.032429555851069304\n",
      "train loss:0.08779581036396288\n",
      "train loss:0.04026788124247254\n",
      "train loss:0.043804264820443396\n",
      "train loss:0.023710802411003744\n",
      "train loss:0.16260456422777536\n",
      "=== epoch:14, train acc:0.976, test acc:0.963 ===\n",
      "train loss:0.07924527754890515\n",
      "train loss:0.089477013141142\n",
      "train loss:0.03508049143459892\n",
      "train loss:0.048001317816287685\n",
      "train loss:0.03253406570212588\n",
      "train loss:0.09759135527370656\n",
      "train loss:0.04165219622650437\n",
      "train loss:0.0889470114174776\n",
      "train loss:0.03678567748593817\n",
      "train loss:0.025530455595172624\n",
      "train loss:0.07107300447431283\n",
      "train loss:0.016891114048895744\n",
      "train loss:0.05547066595643647\n",
      "train loss:0.019524162441041458\n",
      "train loss:0.024558521024175106\n",
      "train loss:0.034285687602824556\n",
      "train loss:0.019210212899994448\n",
      "train loss:0.08240424728449156\n",
      "train loss:0.02920466478287048\n",
      "train loss:0.051172239177423515\n",
      "train loss:0.11950946321672103\n",
      "train loss:0.024058120524501084\n",
      "train loss:0.03539354663095996\n",
      "train loss:0.04249165970780926\n",
      "train loss:0.11583306524313171\n",
      "train loss:0.06124707758412687\n",
      "train loss:0.03463452349088095\n",
      "train loss:0.051649506499745194\n",
      "train loss:0.07497966919538983\n",
      "train loss:0.01672374040644991\n",
      "train loss:0.05272874544797976\n",
      "train loss:0.02439815821973574\n",
      "train loss:0.03961033283824384\n",
      "train loss:0.049611879329531525\n",
      "train loss:0.048826705961486416\n",
      "train loss:0.06719007671154859\n",
      "train loss:0.1255426356345299\n",
      "train loss:0.05557933946761571\n",
      "train loss:0.04072040956653824\n",
      "train loss:0.06082924103830096\n",
      "train loss:0.05923987171420937\n",
      "train loss:0.04294984727855816\n",
      "train loss:0.040847485148672036\n",
      "train loss:0.020760421910459428\n",
      "train loss:0.07361703373601956\n",
      "train loss:0.019629448637317696\n",
      "train loss:0.04776563118737495\n",
      "train loss:0.04261290037490529\n",
      "train loss:0.03751686196944719\n",
      "train loss:0.027777142405842278\n",
      "=== epoch:15, train acc:0.981, test acc:0.964 ===\n",
      "train loss:0.12230133875107219\n",
      "train loss:0.061743776341193615\n",
      "train loss:0.04629032104154093\n",
      "train loss:0.018243326376098264\n",
      "train loss:0.04878017253967287\n",
      "train loss:0.021594267466650253\n",
      "train loss:0.024297118025689485\n",
      "train loss:0.02024696973150895\n",
      "train loss:0.03202093427798327\n",
      "train loss:0.04915102298604784\n",
      "train loss:0.020697503888684837\n",
      "train loss:0.07100557870479532\n",
      "train loss:0.03721624413039121\n",
      "train loss:0.042036416316810474\n",
      "train loss:0.08519434558449245\n",
      "train loss:0.04433848222344599\n",
      "train loss:0.06027966020723177\n",
      "train loss:0.07017069777970394\n",
      "train loss:0.051210130709304205\n",
      "train loss:0.07819711471345812\n",
      "train loss:0.03785969422875221\n",
      "train loss:0.06217318299367012\n",
      "train loss:0.10462099073056676\n",
      "train loss:0.014119987683672867\n",
      "train loss:0.05109853444846935\n",
      "train loss:0.03524945025154054\n",
      "train loss:0.04518183545546553\n",
      "train loss:0.026301899263308233\n",
      "train loss:0.03670720773719016\n",
      "train loss:0.02155929684217796\n",
      "train loss:0.05076113471052467\n",
      "train loss:0.03242949587319532\n",
      "train loss:0.07483154326053619\n",
      "train loss:0.017530934952302974\n",
      "train loss:0.022950452779626963\n",
      "train loss:0.035159164891992056\n",
      "train loss:0.04476346802672955\n",
      "train loss:0.017916212160759552\n",
      "train loss:0.032451809699192764\n",
      "train loss:0.02782618740217545\n",
      "train loss:0.0340772727501556\n",
      "train loss:0.023343200822889657\n",
      "train loss:0.049570472299625294\n",
      "train loss:0.020480546031439565\n",
      "train loss:0.05912211407233072\n",
      "train loss:0.11807238887129312\n",
      "train loss:0.009079796397074173\n",
      "train loss:0.022837298742041528\n",
      "train loss:0.021931668599768184\n",
      "train loss:0.05513316979947175\n",
      "=== epoch:16, train acc:0.975, test acc:0.961 ===\n",
      "train loss:0.020197793737538213\n",
      "train loss:0.06841899178779375\n",
      "train loss:0.011523192721485663\n",
      "train loss:0.06981671906113762\n",
      "train loss:0.06488929605574424\n",
      "train loss:0.043787183510669264\n",
      "train loss:0.04024587955028397\n",
      "train loss:0.02350557885746613\n",
      "train loss:0.026914273327314296\n",
      "train loss:0.04721136142250957\n",
      "train loss:0.0254345868460074\n",
      "train loss:0.09101283276443306\n",
      "train loss:0.13164004326780565\n",
      "train loss:0.026695754976500924\n",
      "train loss:0.06185286092134928\n",
      "train loss:0.027102901860179666\n",
      "train loss:0.05647545761396015\n",
      "train loss:0.023556794709927456\n",
      "train loss:0.09181897597037113\n",
      "train loss:0.034423079690159314\n",
      "train loss:0.03491597329356611\n",
      "train loss:0.07301716525191063\n",
      "train loss:0.046897231943640996\n",
      "train loss:0.016265982830034095\n",
      "train loss:0.07846979533662726\n",
      "train loss:0.06999803530947098\n",
      "train loss:0.021064334359361263\n",
      "train loss:0.03048149950065622\n",
      "train loss:0.07977942559153008\n",
      "train loss:0.08937240601696007\n",
      "train loss:0.16213531256337832\n",
      "train loss:0.06647006473536145\n",
      "train loss:0.026000061863929974\n",
      "train loss:0.01722109210087573\n",
      "train loss:0.05732859776669438\n",
      "train loss:0.023690820440362494\n",
      "train loss:0.039714628131921545\n",
      "train loss:0.025818237441941276\n",
      "train loss:0.08003343021912401\n",
      "train loss:0.030460458134316557\n",
      "train loss:0.030565059867651\n",
      "train loss:0.027012490425389522\n",
      "train loss:0.034754894388789974\n",
      "train loss:0.02015156207911825\n",
      "train loss:0.045435167424682996\n",
      "train loss:0.049579443099016664\n",
      "train loss:0.0789516458801224\n",
      "train loss:0.01203212351569491\n",
      "train loss:0.07684199905229255\n",
      "train loss:0.02779532582347223\n",
      "=== epoch:17, train acc:0.983, test acc:0.964 ===\n",
      "train loss:0.06028831786226782\n",
      "train loss:0.0344363988566692\n",
      "train loss:0.05922198832926187\n",
      "train loss:0.059860925976183366\n",
      "train loss:0.019164487064152892\n",
      "train loss:0.02448636125512206\n",
      "train loss:0.04224886573513551\n",
      "train loss:0.019880035981290976\n",
      "train loss:0.03690370422007376\n",
      "train loss:0.0551346865098814\n",
      "train loss:0.024502602898948567\n",
      "train loss:0.06433482938019192\n",
      "train loss:0.01606248799120685\n",
      "train loss:0.015772749323580134\n",
      "train loss:0.008996777471905398\n",
      "train loss:0.022760386182768384\n",
      "train loss:0.0562794422410048\n",
      "train loss:0.03436075050761317\n",
      "train loss:0.12297270044391372\n",
      "train loss:0.08728508558350427\n",
      "train loss:0.028474254074575395\n",
      "train loss:0.017339286438440994\n",
      "train loss:0.01996761183905579\n",
      "train loss:0.05447396109896721\n",
      "train loss:0.008312774706483122\n",
      "train loss:0.024948163383529006\n",
      "train loss:0.06954469234576557\n",
      "train loss:0.021906678394054843\n",
      "train loss:0.025058780804796954\n",
      "train loss:0.040193633487676184\n",
      "train loss:0.03854910758596343\n",
      "train loss:0.04289259728321244\n",
      "train loss:0.08336729643018469\n",
      "train loss:0.043652026712656465\n",
      "train loss:0.011696165356357011\n",
      "train loss:0.03867044010181035\n",
      "train loss:0.06598073982509872\n",
      "train loss:0.03325564928947264\n",
      "train loss:0.013351033923385212\n",
      "train loss:0.025778489529917868\n",
      "train loss:0.019951337677170434\n",
      "train loss:0.004655043266833682\n",
      "train loss:0.02348249216141458\n",
      "train loss:0.006611802883174879\n",
      "train loss:0.018607761672905104\n",
      "train loss:0.020790187656536934\n",
      "train loss:0.07129599733930904\n",
      "train loss:0.17266233529029484\n",
      "train loss:0.013249231975803894\n",
      "train loss:0.02316189910619868\n",
      "=== epoch:18, train acc:0.985, test acc:0.962 ===\n",
      "train loss:0.025035538791465757\n",
      "train loss:0.033313483793849546\n",
      "train loss:0.017239947690749734\n",
      "train loss:0.02278109347158297\n",
      "train loss:0.03614291137169874\n",
      "train loss:0.021698471782885614\n",
      "train loss:0.013840063442714563\n",
      "train loss:0.04560349537433337\n",
      "train loss:0.07260035049567776\n",
      "train loss:0.02007432187344024\n",
      "train loss:0.011509706012882117\n",
      "train loss:0.018975315390910914\n",
      "train loss:0.08567849151994598\n",
      "train loss:0.013828841167358844\n",
      "train loss:0.03866832134202092\n",
      "train loss:0.007468154977118265\n",
      "train loss:0.009371741691385205\n",
      "train loss:0.07011768314573943\n",
      "train loss:0.07763819314111182\n",
      "train loss:0.03537204816393452\n",
      "train loss:0.011691849501418832\n",
      "train loss:0.020910742328319798\n",
      "train loss:0.01829980033260805\n",
      "train loss:0.040745640141542944\n",
      "train loss:0.017505406827994063\n",
      "train loss:0.013248852993244197\n",
      "train loss:0.014989393487800989\n",
      "train loss:0.035723208793885866\n",
      "train loss:0.020819161347210936\n",
      "train loss:0.027266661551604402\n",
      "train loss:0.019726739759469602\n",
      "train loss:0.04449960747498775\n",
      "train loss:0.06524229319318903\n",
      "train loss:0.010709257601985182\n",
      "train loss:0.027901049557027515\n",
      "train loss:0.033203160360802134\n",
      "train loss:0.007408789227870059\n",
      "train loss:0.1291504673203371\n",
      "train loss:0.03343930835442633\n",
      "train loss:0.060565566267809894\n",
      "train loss:0.056551766606123224\n",
      "train loss:0.035687341536683174\n",
      "train loss:0.06584499651623171\n",
      "train loss:0.04791766158748494\n",
      "train loss:0.05336416226581439\n",
      "train loss:0.07063911084116985\n",
      "train loss:0.020292042459558176\n",
      "train loss:0.02741602521266511\n",
      "train loss:0.049274149097087744\n",
      "train loss:0.051845008341236544\n",
      "=== epoch:19, train acc:0.988, test acc:0.96 ===\n",
      "train loss:0.013862280375535068\n",
      "train loss:0.050351934227870834\n",
      "train loss:0.044942997150943576\n",
      "train loss:0.08218330782698478\n",
      "train loss:0.02642550982311803\n",
      "train loss:0.0235461993320693\n",
      "train loss:0.043947442826435486\n",
      "train loss:0.0254342220639662\n",
      "train loss:0.07432408407832174\n",
      "train loss:0.026713385109631574\n",
      "train loss:0.038385727847107344\n",
      "train loss:0.044770396052641936\n",
      "train loss:0.07971019956031944\n",
      "train loss:0.008956347430747122\n",
      "train loss:0.011035351019140922\n",
      "train loss:0.019839083800278788\n",
      "train loss:0.017178860065005894\n",
      "train loss:0.030116933501929694\n",
      "train loss:0.01087065279146475\n",
      "train loss:0.005258360106504772\n",
      "train loss:0.02411125477129358\n",
      "train loss:0.01283404111771112\n",
      "train loss:0.007702311591985554\n",
      "train loss:0.028428615824989963\n",
      "train loss:0.02328276546097396\n",
      "train loss:0.04787347705534302\n",
      "train loss:0.10017720485331066\n",
      "train loss:0.01011507163160277\n",
      "train loss:0.01858678127399482\n",
      "train loss:0.016376254374452122\n",
      "train loss:0.012621826385344011\n",
      "train loss:0.029220185949202396\n",
      "train loss:0.04752429036573865\n",
      "train loss:0.026321785242468504\n",
      "train loss:0.03925602477628137\n",
      "train loss:0.008299552916031239\n",
      "train loss:0.02620707152113937\n",
      "train loss:0.012242757435570982\n",
      "train loss:0.015269118676550099\n",
      "train loss:0.05949279856515573\n",
      "train loss:0.015185242165054761\n",
      "train loss:0.030680003588350654\n",
      "train loss:0.029333327552624192\n",
      "train loss:0.02109273120588369\n",
      "train loss:0.011117881911519798\n",
      "train loss:0.041591070180068065\n",
      "train loss:0.03739623306184832\n",
      "train loss:0.013273183296332994\n",
      "train loss:0.01645873796261091\n",
      "train loss:0.02738184653712027\n",
      "=== epoch:20, train acc:0.987, test acc:0.962 ===\n",
      "train loss:0.01835144116335882\n",
      "train loss:0.016955908651701313\n",
      "train loss:0.014525633583759509\n",
      "train loss:0.03244284904473513\n",
      "train loss:0.016466417107045207\n",
      "train loss:0.0075148774569052165\n",
      "train loss:0.020799169773286066\n",
      "train loss:0.015459352845625926\n",
      "train loss:0.010394395033093211\n",
      "train loss:0.029792811261714233\n",
      "train loss:0.034352225289419835\n",
      "train loss:0.11125103832940338\n",
      "train loss:0.003549109812576509\n",
      "train loss:0.013762950533330227\n",
      "train loss:0.015090277051266737\n",
      "train loss:0.09208459961062333\n",
      "train loss:0.020972814213102566\n",
      "train loss:0.03467254245860966\n",
      "train loss:0.017808392824697844\n",
      "train loss:0.01734059552365658\n",
      "train loss:0.07471107393558465\n",
      "train loss:0.03965723026410379\n",
      "train loss:0.0426410235994273\n",
      "train loss:0.016462225945913688\n",
      "train loss:0.007297429693017071\n",
      "train loss:0.0394546657007746\n",
      "train loss:0.033262685777694215\n",
      "train loss:0.01906588317687355\n",
      "train loss:0.03464253153807623\n",
      "train loss:0.017253610285024604\n",
      "train loss:0.03585060010172946\n",
      "train loss:0.02927086261548852\n",
      "train loss:0.010937963126953387\n",
      "train loss:0.05918782795326314\n",
      "train loss:0.014848566556016313\n",
      "train loss:0.00865266728921061\n",
      "train loss:0.03011391373504694\n",
      "train loss:0.022613755553425555\n",
      "train loss:0.011921063682703115\n",
      "train loss:0.01925424676276565\n",
      "train loss:0.041727615453817896\n",
      "train loss:0.014959517120641945\n",
      "train loss:0.009612724198692392\n",
      "train loss:0.025635086588460778\n",
      "train loss:0.02156121502308453\n",
      "train loss:0.020852855340110277\n",
      "train loss:0.01562672134093153\n",
      "train loss:0.05108050628883028\n",
      "train loss:0.06246444913573047\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.958\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28), \n",
    "                        conv_param_1={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        conv_param_2={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "network.save_params(\"params_self.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
