{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import common.layers as layers\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 hidden_size1=100, hidden_size2=100, hidden_size3=100,\n",
    "                 output_size=10, weight_init_std=0.01):\n",
    "        \"\"\"\n",
    "        SimpleConvNet을 참고하여 MLP 형태로 만든 클래스.\n",
    "        파라미터는 W1~W4, b1~b4 총 4세트를 사용.\n",
    "        \n",
    "        구조:\n",
    "            (Affine1 -> Relu1) ->\n",
    "            (Affine2 -> Relu2) ->\n",
    "            (Affine3 -> Relu3) ->\n",
    "            (Affine4 -> Softmax)\n",
    "        \n",
    "        input_dim  : (채널=1, 높이=28, 너비=28)\n",
    "        hidden_size1,2,3 : 세 개의 은닉층 크기\n",
    "        output_size : 최종 출력 차원(디폴트 10, MNIST용)\n",
    "        weight_init_std : 가중치 초기화 스케일\n",
    "        \"\"\"\n",
    "        # 입력을 펼쳤을 때의 차원 (1*28*28 = 784)\n",
    "        input_size = input_dim[0] * input_dim[1] * input_dim[2]\n",
    "        \n",
    "        # 모든 파라미터를 딕셔너리에 저장\n",
    "        self.params = {}\n",
    "        \n",
    "        # 1) W1, b1 (Affine1)\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)\n",
    "        self.params['b1'] = np.zeros(hidden_size1)\n",
    "        \n",
    "        # 2) W2, b2 (Affine2)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, hidden_size2)\n",
    "        self.params['b2'] = np.zeros(hidden_size2)\n",
    "        \n",
    "        # 3) W3, b3 (Affine3)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, hidden_size3)\n",
    "        self.params['b3'] = np.zeros(hidden_size3)\n",
    "        \n",
    "        # 4) W4, b4 (Affine4)\n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size3, output_size)\n",
    "        self.params['b4'] = np.zeros(output_size)\n",
    "        \n",
    "        # ---------------------------------------------------\n",
    "        # 계층(Layers)들을 순서대로 쌓기\n",
    "        # ---------------------------------------------------\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = layers.Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1']   = layers.Relu()\n",
    "        self.layers['Affine2'] = layers.Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2']   = layers.Relu()\n",
    "        self.layers['Affine3'] = layers.Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['Relu3']   = layers.Relu()\n",
    "        self.layers['Affine4'] = layers.Affine(self.params['W4'], self.params['b4'])\n",
    "        \n",
    "        # 마지막 계층: 소프트맥스 + 크로스 엔트로피 손실\n",
    "        self.last_layer = layers.SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        순전파(추론) 메서드\n",
    "        x: (배치크기, 1, 28, 28) 형태라고 가정\n",
    "        \"\"\"\n",
    "        # 2D로 펼치기: (batch_size, 784)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        # 순서대로 레이어 Forward\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        손실 함수(크로스 엔트로피) 계산\n",
    "        x: 입력데이터\n",
    "        t: 정답레이블(원-핫 or 정수 라벨 모두 가능; SoftmaxWithLoss 구현에 따라 다름)\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        정확도 측정\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "\n",
    "        # t(정답)가 원-핫 벡터일 경우 argmax로 정수라벨 추출\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        오차역전파를 통해 각 파라미터의 기울기를 구함\n",
    "        반환: grads 딕셔너리 (W1, b1, W2, b2, W3, b3, W4, b4)\n",
    "        \"\"\"\n",
    "        # 1) 순전파 (loss 계산)\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 2) 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)  # SoftmaxWithLoss 거꾸로\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 3) 각 파라미터의 기울기를 grads에 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "        grads['W4'], grads['b4'] = self.layers['Affine4'].dW, self.layers['Affine4'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        파라미터(가중치, 편향) 저장\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        파라미터(가중치, 편향) 로드\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        # 로드한 파라미터를 실제 레이어에 반영\n",
    "        for i, key in enumerate(['Affine1', 'Affine2', 'Affine3', 'Affine4']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.302177286049988\n",
      "=== epoch:1, train acc:0.087, test acc:0.089 ===\n",
      "train loss:2.302413729631005\n",
      "train loss:2.3024404526006332\n",
      "train loss:2.3022028942253754\n",
      "train loss:2.3017414806692345\n",
      "train loss:2.3018765069572735\n",
      "train loss:2.3014807384300373\n",
      "train loss:2.3004735462747945\n",
      "train loss:2.3002413552812255\n",
      "train loss:2.2983352989972654\n",
      "train loss:2.2955754536762045\n",
      "train loss:2.292730299679528\n",
      "train loss:2.289624627027066\n",
      "train loss:2.2851550697809087\n",
      "train loss:2.2784489096306753\n",
      "train loss:2.268981895013188\n",
      "train loss:2.2689887974373604\n",
      "train loss:2.2497725352763096\n",
      "train loss:2.234085271052611\n",
      "train loss:2.21601219348631\n",
      "train loss:2.189254527852499\n",
      "train loss:2.1660681665683397\n",
      "train loss:2.1729794483140754\n",
      "train loss:2.0958575016985828\n",
      "train loss:2.10279939308742\n",
      "train loss:2.024423298634274\n",
      "train loss:2.0062818823005766\n",
      "train loss:1.9045457404621684\n",
      "train loss:1.8993744790075175\n",
      "train loss:1.7790487602506053\n",
      "train loss:1.764213950488266\n",
      "train loss:1.7108051912070426\n",
      "train loss:1.6184923657212489\n",
      "train loss:1.5421642368187714\n",
      "train loss:1.4642239061007882\n",
      "train loss:1.39440291478853\n",
      "train loss:1.3990365539902718\n",
      "train loss:1.3707888267709476\n",
      "train loss:1.3536551216505537\n",
      "train loss:1.3129874181033583\n",
      "train loss:1.2398327061506622\n",
      "train loss:1.347261293316707\n",
      "train loss:1.2465160054007964\n",
      "train loss:1.1541896224828598\n",
      "train loss:1.1402386973740106\n",
      "train loss:1.191888298345221\n",
      "train loss:1.0311776747294117\n",
      "train loss:1.1011088772547368\n",
      "train loss:1.1029918257942544\n",
      "train loss:1.1240230396221018\n",
      "train loss:1.1217648899972223\n",
      "=== epoch:2, train acc:0.634, test acc:0.592 ===\n",
      "train loss:1.205966505057859\n",
      "train loss:0.999145881553911\n",
      "train loss:1.0050839483535254\n",
      "train loss:0.9547159398157603\n",
      "train loss:0.9161417424246596\n",
      "train loss:0.9498765199026259\n",
      "train loss:1.2067370027079007\n",
      "train loss:0.9281437996929159\n",
      "train loss:0.9886598854366002\n",
      "train loss:0.9688921225473213\n",
      "train loss:0.9848395858806742\n",
      "train loss:1.033524026541272\n",
      "train loss:1.0768566859333304\n",
      "train loss:1.2374035592697468\n",
      "train loss:1.1018542344606177\n",
      "train loss:1.1091164370087137\n",
      "train loss:0.9163847228273322\n",
      "train loss:0.8778504817594466\n",
      "train loss:0.8649110400345792\n",
      "train loss:1.1071957712994454\n",
      "train loss:0.8097150895528366\n",
      "train loss:0.717901000660173\n",
      "train loss:0.9971098929560008\n",
      "train loss:0.9313042507114715\n",
      "train loss:0.952010561385192\n",
      "train loss:0.9280577755894293\n",
      "train loss:0.8124837770321338\n",
      "train loss:0.8275815296140212\n",
      "train loss:0.8568922909032498\n",
      "train loss:0.7654818899436471\n",
      "train loss:0.9867218038957154\n",
      "train loss:0.9027112406508627\n",
      "train loss:0.8872255418918641\n",
      "train loss:0.7531369358965517\n",
      "train loss:0.9457827261922076\n",
      "train loss:0.6623659631252649\n",
      "train loss:0.7087464831870482\n",
      "train loss:0.8673022635557603\n",
      "train loss:0.8185911014627238\n",
      "train loss:0.8621733371752549\n",
      "train loss:0.7397022288121411\n",
      "train loss:0.7318737618973489\n",
      "train loss:0.8964333134147747\n",
      "train loss:0.853547564796193\n",
      "train loss:0.7254562927664341\n",
      "train loss:0.9086487415603596\n",
      "train loss:1.1292022396094736\n",
      "train loss:0.829196440412789\n",
      "train loss:0.8773379502164096\n",
      "train loss:0.7491734828877245\n",
      "=== epoch:3, train acc:0.707, test acc:0.662 ===\n",
      "train loss:0.7191297755603034\n",
      "train loss:0.6789226183395417\n",
      "train loss:0.912527759097516\n",
      "train loss:0.7663420252900432\n",
      "train loss:0.7742276169162287\n",
      "train loss:0.688734914651848\n",
      "train loss:0.7214043802912403\n",
      "train loss:0.8665071908010908\n",
      "train loss:0.8041050081178807\n",
      "train loss:0.9813314693866453\n",
      "train loss:0.7243569465297274\n",
      "train loss:0.7582836094988217\n",
      "train loss:0.8843038367354071\n",
      "train loss:0.8064326154948068\n",
      "train loss:0.6960837250302119\n",
      "train loss:0.8387499946620003\n",
      "train loss:0.741458644621881\n",
      "train loss:0.9527492930277576\n",
      "train loss:0.6316648975626594\n",
      "train loss:0.7837459214752696\n",
      "train loss:0.8193520748751069\n",
      "train loss:0.6624425791248041\n",
      "train loss:0.9065078169146661\n",
      "train loss:0.7062169526988349\n",
      "train loss:0.5839850787141129\n",
      "train loss:0.750729012480419\n",
      "train loss:0.7520938984657412\n",
      "train loss:0.8038837708324345\n",
      "train loss:0.7042849778789914\n",
      "train loss:0.8085691155516578\n",
      "train loss:0.793188398369346\n",
      "train loss:0.659337446243774\n",
      "train loss:0.6468276387930723\n",
      "train loss:0.81112903839331\n",
      "train loss:0.6213629111150492\n",
      "train loss:0.8160245356214064\n",
      "train loss:0.6619928497366717\n",
      "train loss:0.6882783245382952\n",
      "train loss:0.6012887756444625\n",
      "train loss:0.712607251217432\n",
      "train loss:0.6195399306921603\n",
      "train loss:0.49854300825062675\n",
      "train loss:0.7175211614533664\n",
      "train loss:0.6456555337848884\n",
      "train loss:0.7957075865078198\n",
      "train loss:0.6287987096210648\n",
      "train loss:0.6801946775268266\n",
      "train loss:0.7817988374707362\n",
      "train loss:0.6315575419504516\n",
      "train loss:0.647611553386994\n",
      "=== epoch:4, train acc:0.796, test acc:0.768 ===\n",
      "train loss:0.5809906714440877\n",
      "train loss:0.5106519663165029\n",
      "train loss:0.5362087172536006\n",
      "train loss:0.5631620865174236\n",
      "train loss:0.8018473174371235\n",
      "train loss:0.4955313906124218\n",
      "train loss:0.6091548771143508\n",
      "train loss:0.41691414036748475\n",
      "train loss:0.6005100748681094\n",
      "train loss:0.3919613023221246\n",
      "train loss:0.560733861842107\n",
      "train loss:0.6529907307219738\n",
      "train loss:0.7264235109396661\n",
      "train loss:0.582863517048576\n",
      "train loss:0.71792497529512\n",
      "train loss:0.7232217525966255\n",
      "train loss:0.7344015772290008\n",
      "train loss:0.6487672646708704\n",
      "train loss:0.5079707353372362\n",
      "train loss:0.5653133246178655\n",
      "train loss:0.5531254207458454\n",
      "train loss:0.6115135695236787\n",
      "train loss:0.556328145242259\n",
      "train loss:0.7489822909523103\n",
      "train loss:0.493870520111894\n",
      "train loss:0.46284752181163086\n",
      "train loss:0.6434922129158245\n",
      "train loss:0.6542637156747255\n",
      "train loss:0.741637334214687\n",
      "train loss:0.7876326820778605\n",
      "train loss:0.6051123483644969\n",
      "train loss:0.6649261565137679\n",
      "train loss:0.48713196424385935\n",
      "train loss:0.5215745836834332\n",
      "train loss:0.4098062058933558\n",
      "train loss:0.575857240977044\n",
      "train loss:0.6730790368404898\n",
      "train loss:0.7689227377393207\n",
      "train loss:0.6044139901440336\n",
      "train loss:0.6659028946018937\n",
      "train loss:0.5177541083094167\n",
      "train loss:0.4224394357166939\n",
      "train loss:0.5608779810805733\n",
      "train loss:0.3920269465763427\n",
      "train loss:0.6469382574683089\n",
      "train loss:0.6829846328020297\n",
      "train loss:0.4639171752511105\n",
      "train loss:0.5885101805187175\n",
      "train loss:0.5915780213606028\n",
      "train loss:0.41417151240020256\n",
      "=== epoch:5, train acc:0.816, test acc:0.803 ===\n",
      "train loss:0.5461956648596098\n",
      "train loss:0.5800351279549762\n",
      "train loss:0.5338065182359614\n",
      "train loss:0.402964330529275\n",
      "train loss:0.4588961364393039\n",
      "train loss:0.4719360848755012\n",
      "train loss:0.5236485883343484\n",
      "train loss:0.4855160951529723\n",
      "train loss:0.4109707664367503\n",
      "train loss:0.4971547124140646\n",
      "train loss:0.3584236190316943\n",
      "train loss:0.4854296117677203\n",
      "train loss:0.4300732228880847\n",
      "train loss:0.4852417168981956\n",
      "train loss:0.45840271631657165\n",
      "train loss:0.5687260883995546\n",
      "train loss:0.5194398962637272\n",
      "train loss:0.4881705274079922\n",
      "train loss:0.5952715914165658\n",
      "train loss:0.5382101918059371\n",
      "train loss:0.3934409849362095\n",
      "train loss:0.3478954322637094\n",
      "train loss:0.4743639598252636\n",
      "train loss:0.4317352794450357\n",
      "train loss:0.5861793676232783\n",
      "train loss:0.43479564223190015\n",
      "train loss:0.38302307354412823\n",
      "train loss:0.7619811072861803\n",
      "train loss:0.4815300454377207\n",
      "train loss:0.5269464689986306\n",
      "train loss:0.4850791682058924\n",
      "train loss:0.5426076529217911\n",
      "train loss:0.46426567957976084\n",
      "train loss:0.3284784854597779\n",
      "train loss:0.35374136020832586\n",
      "train loss:0.38063413350054454\n",
      "train loss:0.29175662069869773\n",
      "train loss:0.3206672077265423\n",
      "train loss:0.44668859988614235\n",
      "train loss:0.4838969653932937\n",
      "train loss:0.4330000114496599\n",
      "train loss:0.30875658722209903\n",
      "train loss:0.44171036399436336\n",
      "train loss:0.5651412337581658\n",
      "train loss:0.4121039503048898\n",
      "train loss:0.4314716205614927\n",
      "train loss:0.5097929098740938\n",
      "train loss:0.44667869079640804\n",
      "train loss:0.40523609847397674\n",
      "train loss:0.6382639593786557\n",
      "=== epoch:6, train acc:0.852, test acc:0.809 ===\n",
      "train loss:0.4866423785916145\n",
      "train loss:0.5107565944820522\n",
      "train loss:0.47152776984133077\n",
      "train loss:0.4137608987317003\n",
      "train loss:0.4827722112612304\n",
      "train loss:0.3665160951692993\n",
      "train loss:0.35405846607279956\n",
      "train loss:0.3567766633522087\n",
      "train loss:0.4997491684311053\n",
      "train loss:0.48863528393073885\n",
      "train loss:0.4770087983973174\n",
      "train loss:0.5666748645793164\n",
      "train loss:0.35248106676335345\n",
      "train loss:0.5322952085310655\n",
      "train loss:0.4227487266961636\n",
      "train loss:0.6105336431809388\n",
      "train loss:0.5592293209891821\n",
      "train loss:0.36147062773640143\n",
      "train loss:0.5260964727169523\n",
      "train loss:0.4661124486051111\n",
      "train loss:0.2597995431664291\n",
      "train loss:0.3552947985105168\n",
      "train loss:0.40836983611536\n",
      "train loss:0.3727410730902277\n",
      "train loss:0.37064126472168774\n",
      "train loss:0.3975857372783323\n",
      "train loss:0.37218861869739955\n",
      "train loss:0.3611264845091937\n",
      "train loss:0.43981317623308946\n",
      "train loss:0.399682099268163\n",
      "train loss:0.39947787880583036\n",
      "train loss:0.2768449716111698\n",
      "train loss:0.35829970322570787\n",
      "train loss:0.3479090138557421\n",
      "train loss:0.30056134692511854\n",
      "train loss:0.4061098594185537\n",
      "train loss:0.41937267300651415\n",
      "train loss:0.45617227344733957\n",
      "train loss:0.48634097208678\n",
      "train loss:0.36438871884562596\n",
      "train loss:0.3810459891792408\n",
      "train loss:0.4911862203943356\n",
      "train loss:0.310137922702599\n",
      "train loss:0.33619054422728895\n",
      "train loss:0.33738461664293173\n",
      "train loss:0.27704129990991444\n",
      "train loss:0.3876785818926814\n",
      "train loss:0.4781310749446666\n",
      "train loss:0.3679570372305878\n",
      "train loss:0.44943474272533174\n",
      "=== epoch:7, train acc:0.857, test acc:0.832 ===\n",
      "train loss:0.42743434364943367\n",
      "train loss:0.459456518885877\n",
      "train loss:0.3199367603430783\n",
      "train loss:0.39857655482466453\n",
      "train loss:0.7429164863702182\n",
      "train loss:0.3585399044335124\n",
      "train loss:0.34629584397683766\n",
      "train loss:0.2933240262679361\n",
      "train loss:0.30412793230282487\n",
      "train loss:0.4783893067800962\n",
      "train loss:0.36990742514476405\n",
      "train loss:0.3670802614132661\n",
      "train loss:0.3860307934163714\n",
      "train loss:0.5266121267173252\n",
      "train loss:0.4140585271342781\n",
      "train loss:0.29560030462647624\n",
      "train loss:0.31441731825445596\n",
      "train loss:0.359201603042335\n",
      "train loss:0.4342119772551133\n",
      "train loss:0.3116828925004828\n",
      "train loss:0.42989125780329124\n",
      "train loss:0.3338343569655863\n",
      "train loss:0.6563218507961914\n",
      "train loss:0.343581462898579\n",
      "train loss:0.369870010003159\n",
      "train loss:0.29796568263131096\n",
      "train loss:0.30917323688606096\n",
      "train loss:0.24822258473959974\n",
      "train loss:0.34729760928662295\n",
      "train loss:0.39043848182568625\n",
      "train loss:0.484529925227089\n",
      "train loss:0.4762021096854373\n",
      "train loss:0.34334887986499024\n",
      "train loss:0.2872839391148473\n",
      "train loss:0.3073597627305119\n",
      "train loss:0.32772761029618896\n",
      "train loss:0.47508980293290953\n",
      "train loss:0.3617371674961649\n",
      "train loss:0.2618692597320805\n",
      "train loss:0.27221840942420156\n",
      "train loss:0.23011625541615416\n",
      "train loss:0.3831212422547848\n",
      "train loss:0.3439177182209592\n",
      "train loss:0.3855915499825306\n",
      "train loss:0.3687821452177738\n",
      "train loss:0.3871696887105995\n",
      "train loss:0.24037341248304223\n",
      "train loss:0.29272355601258626\n",
      "train loss:0.49295621993972033\n",
      "train loss:0.37189405081552507\n",
      "=== epoch:8, train acc:0.875, test acc:0.821 ===\n",
      "train loss:0.42566914668051853\n",
      "train loss:0.518615038225986\n",
      "train loss:0.167858582842631\n",
      "train loss:0.3859873300497304\n",
      "train loss:0.25795877996801203\n",
      "train loss:0.32893680854525714\n",
      "train loss:0.2523829069982375\n",
      "train loss:0.3046562054817071\n",
      "train loss:0.6646737527612236\n",
      "train loss:0.5422341961013512\n",
      "train loss:0.3142636106983457\n",
      "train loss:0.24209715971807125\n",
      "train loss:0.42532954936588746\n",
      "train loss:0.41605910491913994\n",
      "train loss:0.43198298492410514\n",
      "train loss:0.29885785229133666\n",
      "train loss:0.32416918683313767\n",
      "train loss:0.3862330470800094\n",
      "train loss:0.3728094916112212\n",
      "train loss:0.20536068062419638\n",
      "train loss:0.5353111732398105\n",
      "train loss:0.27166306389840517\n",
      "train loss:0.2616459372787902\n",
      "train loss:0.43463668074855166\n",
      "train loss:0.3652439754217003\n",
      "train loss:0.23695945116726883\n",
      "train loss:0.3743745773124846\n",
      "train loss:0.32769387147054696\n",
      "train loss:0.36045526754470963\n",
      "train loss:0.4511817374768015\n",
      "train loss:0.5116640722427271\n",
      "train loss:0.31651968683702836\n",
      "train loss:0.23475524203459608\n",
      "train loss:0.26622111760717887\n",
      "train loss:0.26618295220352034\n",
      "train loss:0.37316877256616804\n",
      "train loss:0.3929342808703569\n",
      "train loss:0.2802151661994521\n",
      "train loss:0.4195262079142785\n",
      "train loss:0.42571934473347645\n",
      "train loss:0.43882242380704123\n",
      "train loss:0.22636243527311187\n",
      "train loss:0.2728886549735269\n",
      "train loss:0.3403418299422889\n",
      "train loss:0.4761663653161474\n",
      "train loss:0.48276437897142743\n",
      "train loss:0.2763574226716931\n",
      "train loss:0.40309224557651113\n",
      "train loss:0.3714149664210945\n",
      "train loss:0.3228287174764188\n",
      "=== epoch:9, train acc:0.888, test acc:0.842 ===\n",
      "train loss:0.3796144593138869\n",
      "train loss:0.41116643863942953\n",
      "train loss:0.22142569497063413\n",
      "train loss:0.2601755325143721\n",
      "train loss:0.34739690678347657\n",
      "train loss:0.29643822890197996\n",
      "train loss:0.2502802526320367\n",
      "train loss:0.4511076411242418\n",
      "train loss:0.4085018696277867\n",
      "train loss:0.4315148477910021\n",
      "train loss:0.2480644880845916\n",
      "train loss:0.303292590284354\n",
      "train loss:0.3765282705060739\n",
      "train loss:0.18949176875547954\n",
      "train loss:0.4538174875983637\n",
      "train loss:0.3248816482885966\n",
      "train loss:0.4303767686519195\n",
      "train loss:0.25044768577680776\n",
      "train loss:0.21243612424512592\n",
      "train loss:0.3432791607983694\n",
      "train loss:0.4237638333467197\n",
      "train loss:0.27726808529907115\n",
      "train loss:0.1917395139246655\n",
      "train loss:0.2825398668076421\n",
      "train loss:0.3062068150032366\n",
      "train loss:0.24880672045018526\n",
      "train loss:0.2758010905878596\n",
      "train loss:0.32626121016986187\n",
      "train loss:0.34116648862993487\n",
      "train loss:0.40927201462574436\n",
      "train loss:0.4063418845212844\n",
      "train loss:0.2545700894650605\n",
      "train loss:0.23458970376897825\n",
      "train loss:0.19611133780953183\n",
      "train loss:0.24787128184263035\n",
      "train loss:0.34874798428068693\n",
      "train loss:0.3888754280619198\n",
      "train loss:0.29473084665982796\n",
      "train loss:0.3161612063843328\n",
      "train loss:0.30818123801114117\n",
      "train loss:0.2503201853591609\n",
      "train loss:0.40612207315537013\n",
      "train loss:0.41760437329104955\n",
      "train loss:0.29787770800410857\n",
      "train loss:0.2763561344819844\n",
      "train loss:0.3475984869493612\n",
      "train loss:0.22965739224115492\n",
      "train loss:0.2791425479067521\n",
      "train loss:0.2963447259606411\n",
      "train loss:0.36542836188720507\n",
      "=== epoch:10, train acc:0.906, test acc:0.843 ===\n",
      "train loss:0.2582046904195829\n",
      "train loss:0.29571631005383564\n",
      "train loss:0.22154937355949753\n",
      "train loss:0.25573772812763435\n",
      "train loss:0.1416580816112479\n",
      "train loss:0.2153296854632484\n",
      "train loss:0.23186552281942924\n",
      "train loss:0.3914828818884766\n",
      "train loss:0.5862877413364231\n",
      "train loss:0.32367486157416303\n",
      "train loss:0.30560519929097024\n",
      "train loss:0.3368833951614219\n",
      "train loss:0.19534095307619798\n",
      "train loss:0.270862977466317\n",
      "train loss:0.3966950227038112\n",
      "train loss:0.3294890720292616\n",
      "train loss:0.26633864776807825\n",
      "train loss:0.42304756138478355\n",
      "train loss:0.3205660960141302\n",
      "train loss:0.21559689154002867\n",
      "train loss:0.40205650698577017\n",
      "train loss:0.27153323504738175\n",
      "train loss:0.28280576878960517\n",
      "train loss:0.36069721896881285\n",
      "train loss:0.34164025793153796\n",
      "train loss:0.38903505601875066\n",
      "train loss:0.2944996656957366\n",
      "train loss:0.26711788899553673\n",
      "train loss:0.40246432047242137\n",
      "train loss:0.22729550443918595\n",
      "train loss:0.4905341643373678\n",
      "train loss:0.1921393221660569\n",
      "train loss:0.2871446809730929\n",
      "train loss:0.37052247010214984\n",
      "train loss:0.32801678422884883\n",
      "train loss:0.28306964234984583\n",
      "train loss:0.16647811326418027\n",
      "train loss:0.3116129867541523\n",
      "train loss:0.28607882377586785\n",
      "train loss:0.26154566988965994\n",
      "train loss:0.32835393061625134\n",
      "train loss:0.2578177515943548\n",
      "train loss:0.23500754691132605\n",
      "train loss:0.23683679097873586\n",
      "train loss:0.26363116797748043\n",
      "train loss:0.2517670238218938\n",
      "train loss:0.28586018819000786\n",
      "train loss:0.30024391726132665\n",
      "train loss:0.24959375761983174\n",
      "train loss:0.27431587474563335\n",
      "=== epoch:11, train acc:0.896, test acc:0.852 ===\n",
      "train loss:0.2632568607904904\n",
      "train loss:0.2822125675322828\n",
      "train loss:0.30726409653567655\n",
      "train loss:0.2632353113002556\n",
      "train loss:0.4053274352977465\n",
      "train loss:0.26116929377198644\n",
      "train loss:0.2777858294765607\n",
      "train loss:0.16948824819281813\n",
      "train loss:0.29573049609400276\n",
      "train loss:0.28800696113646596\n",
      "train loss:0.2234049753272675\n",
      "train loss:0.19421367059251007\n",
      "train loss:0.3206035120693519\n",
      "train loss:0.3058040195997655\n",
      "train loss:0.3780013963159304\n",
      "train loss:0.2433676892378967\n",
      "train loss:0.23550638787446837\n",
      "train loss:0.20692544620786568\n",
      "train loss:0.204974560059694\n",
      "train loss:0.2965139918326473\n",
      "train loss:0.335004395043315\n",
      "train loss:0.37614967359846885\n",
      "train loss:0.1854061451405509\n",
      "train loss:0.2225206765951453\n",
      "train loss:0.19206900941792868\n",
      "train loss:0.1616338377822369\n",
      "train loss:0.24730173251257753\n",
      "train loss:0.42620482087322037\n",
      "train loss:0.3222603767996726\n",
      "train loss:0.4554918723745691\n",
      "train loss:0.23426183977414194\n",
      "train loss:0.29544588638385055\n",
      "train loss:0.3419390153322263\n",
      "train loss:0.2064898115475663\n",
      "train loss:0.2212344869932271\n",
      "train loss:0.18332137077979194\n",
      "train loss:0.20618935020924944\n",
      "train loss:0.30847802693713827\n",
      "train loss:0.19254892128492965\n",
      "train loss:0.316953236295337\n",
      "train loss:0.20911730498541226\n",
      "train loss:0.22081739911460344\n",
      "train loss:0.10422754423100981\n",
      "train loss:0.2676607251676783\n",
      "train loss:0.3356085736489662\n",
      "train loss:0.22879841826707306\n",
      "train loss:0.21511697175240913\n",
      "train loss:0.20641327734063725\n",
      "train loss:0.27536696361171326\n",
      "train loss:0.2904196362949567\n",
      "=== epoch:12, train acc:0.919, test acc:0.86 ===\n",
      "train loss:0.2299152303008478\n",
      "train loss:0.2032233384153934\n",
      "train loss:0.22579509480962787\n",
      "train loss:0.27361720857346244\n",
      "train loss:0.2736955235632214\n",
      "train loss:0.17473450731051063\n",
      "train loss:0.19448397072738033\n",
      "train loss:0.1828803150337766\n",
      "train loss:0.308980527345703\n",
      "train loss:0.24508615785827814\n",
      "train loss:0.27555199774513156\n",
      "train loss:0.4547425230051892\n",
      "train loss:0.21481585850811466\n",
      "train loss:0.21918738305004726\n",
      "train loss:0.26421192283343525\n",
      "train loss:0.5609407163539568\n",
      "train loss:0.3739669942507974\n",
      "train loss:0.35358843163303866\n",
      "train loss:0.32513430964506346\n",
      "train loss:0.20072850949528334\n",
      "train loss:0.31514303922846787\n",
      "train loss:0.22926890674825237\n",
      "train loss:0.34780192582259595\n",
      "train loss:0.3997366103968063\n",
      "train loss:0.2396607228363664\n",
      "train loss:0.18755918321647347\n",
      "train loss:0.21925014756255187\n",
      "train loss:0.4271685762639183\n",
      "train loss:0.26374847803991225\n",
      "train loss:0.2708745251826968\n",
      "train loss:0.23536878163157315\n",
      "train loss:0.27416491445508684\n",
      "train loss:0.2082617753364873\n",
      "train loss:0.14964524701918686\n",
      "train loss:0.26023389552185433\n",
      "train loss:0.2718122062236409\n",
      "train loss:0.28680303794684603\n",
      "train loss:0.2315726739997274\n",
      "train loss:0.3272845104201428\n",
      "train loss:0.2835245091963701\n",
      "train loss:0.20667608990320588\n",
      "train loss:0.31117625026985707\n",
      "train loss:0.1778090990356437\n",
      "train loss:0.27788354857717473\n",
      "train loss:0.30467855525744825\n",
      "train loss:0.1514062884776982\n",
      "train loss:0.3203291579030445\n",
      "train loss:0.2935283123880739\n",
      "train loss:0.2730244831923343\n",
      "train loss:0.12557754635300072\n",
      "=== epoch:13, train acc:0.921, test acc:0.849 ===\n",
      "train loss:0.13281931533632046\n",
      "train loss:0.29388118413818526\n",
      "train loss:0.2461111024522562\n",
      "train loss:0.25932171808871446\n",
      "train loss:0.30360985645312794\n",
      "train loss:0.15711549033304156\n",
      "train loss:0.25859055055595515\n",
      "train loss:0.17214285083274106\n",
      "train loss:0.2648224851158563\n",
      "train loss:0.18568986139379237\n",
      "train loss:0.1670457979195194\n",
      "train loss:0.18910157522078477\n",
      "train loss:0.10921317484148915\n",
      "train loss:0.22309486714801985\n",
      "train loss:0.28648156742490877\n",
      "train loss:0.26792821866150746\n",
      "train loss:0.22185636968925962\n",
      "train loss:0.29204617214618844\n",
      "train loss:0.17409279098890312\n",
      "train loss:0.17323156562253073\n",
      "train loss:0.28447578383233957\n",
      "train loss:0.14462771721291046\n",
      "train loss:0.44668467767113657\n",
      "train loss:0.303119877676572\n",
      "train loss:0.20846959199598014\n",
      "train loss:0.3295299997202963\n",
      "train loss:0.34373098732135937\n",
      "train loss:0.299403851829048\n",
      "train loss:0.16984324237195214\n",
      "train loss:0.21964133084826995\n",
      "train loss:0.1195386608536445\n",
      "train loss:0.24793504263616764\n",
      "train loss:0.19796297425562293\n",
      "train loss:0.17854812885106677\n",
      "train loss:0.18256905143973515\n",
      "train loss:0.2568504219917101\n",
      "train loss:0.19515054565554377\n",
      "train loss:0.26164917551222855\n",
      "train loss:0.26459655804167714\n",
      "train loss:0.2315619689019041\n",
      "train loss:0.285332399429708\n",
      "train loss:0.28523395637641397\n",
      "train loss:0.1425862132087301\n",
      "train loss:0.13536166375403863\n",
      "train loss:0.294123747024653\n",
      "train loss:0.21921626106214848\n",
      "train loss:0.20009806069081876\n",
      "train loss:0.1442553623866862\n",
      "train loss:0.2511514777093888\n",
      "train loss:0.17951910445199945\n",
      "=== epoch:14, train acc:0.929, test acc:0.861 ===\n",
      "train loss:0.2675228375857602\n",
      "train loss:0.20209132451028808\n",
      "train loss:0.24555633961792556\n",
      "train loss:0.18378466846496358\n",
      "train loss:0.1367010938502197\n",
      "train loss:0.1638743212586127\n",
      "train loss:0.21549067533963645\n",
      "train loss:0.16112229882467294\n",
      "train loss:0.21684831639187574\n",
      "train loss:0.23866463343657426\n",
      "train loss:0.1521828587599717\n",
      "train loss:0.20322812656145822\n",
      "train loss:0.2063928994040896\n",
      "train loss:0.20904195986171561\n",
      "train loss:0.1724721535883609\n",
      "train loss:0.14163159215874602\n",
      "train loss:0.2101309109271625\n",
      "train loss:0.2555160012226706\n",
      "train loss:0.2507858548863376\n",
      "train loss:0.3606814663868231\n",
      "train loss:0.18520435273821956\n",
      "train loss:0.23669879149835854\n",
      "train loss:0.2634022387920332\n",
      "train loss:0.20089665194617295\n",
      "train loss:0.2145262226520465\n",
      "train loss:0.37586571301275457\n",
      "train loss:0.12780808127272889\n",
      "train loss:0.16962944541337804\n",
      "train loss:0.18686979083472147\n",
      "train loss:0.2248673431760682\n",
      "train loss:0.15143008689576673\n",
      "train loss:0.15746914925386865\n",
      "train loss:0.11164011425493438\n",
      "train loss:0.20136014443959677\n",
      "train loss:0.17483636252215973\n",
      "train loss:0.19712118476320686\n",
      "train loss:0.29219213532265104\n",
      "train loss:0.13330982401089314\n",
      "train loss:0.16903809542602183\n",
      "train loss:0.32101146907392786\n",
      "train loss:0.19156464581667032\n",
      "train loss:0.2533797156811439\n",
      "train loss:0.08122981980876556\n",
      "train loss:0.17175180530813827\n",
      "train loss:0.1293903412103965\n",
      "train loss:0.23280157643628688\n",
      "train loss:0.11140075967367291\n",
      "train loss:0.1697032182974784\n",
      "train loss:0.1815195273668328\n",
      "train loss:0.26051365820402794\n",
      "=== epoch:15, train acc:0.93, test acc:0.861 ===\n",
      "train loss:0.3171553533817585\n",
      "train loss:0.2521698794279535\n",
      "train loss:0.21029244727787483\n",
      "train loss:0.1595992071226019\n",
      "train loss:0.26361733928393494\n",
      "train loss:0.1266975886785271\n",
      "train loss:0.108297632496733\n",
      "train loss:0.1350379793255717\n",
      "train loss:0.12013488301417947\n",
      "train loss:0.22286712549097148\n",
      "train loss:0.21468277089759305\n",
      "train loss:0.13465774035202557\n",
      "train loss:0.1584074376808881\n",
      "train loss:0.25183419421406233\n",
      "train loss:0.11922031129936013\n",
      "train loss:0.20830401409200938\n",
      "train loss:0.1726156020425942\n",
      "train loss:0.20138843555066008\n",
      "train loss:0.1421147541165912\n",
      "train loss:0.22923024709156306\n",
      "train loss:0.18259600904435516\n",
      "train loss:0.12182499115200848\n",
      "train loss:0.21292846166260876\n",
      "train loss:0.14221680995347163\n",
      "train loss:0.16946171424553685\n",
      "train loss:0.12848773643642392\n",
      "train loss:0.13286844941012027\n",
      "train loss:0.2007274016765138\n",
      "train loss:0.11550083659455808\n",
      "train loss:0.18697166454925265\n",
      "train loss:0.12188878874756527\n",
      "train loss:0.18174534198388184\n",
      "train loss:0.1636265916713047\n",
      "train loss:0.211021853856965\n",
      "train loss:0.08378217034739492\n",
      "train loss:0.1760924721797297\n",
      "train loss:0.2213261912682552\n",
      "train loss:0.07430814399906899\n",
      "train loss:0.2094391308685703\n",
      "train loss:0.07810554563608187\n",
      "train loss:0.3142494029411602\n",
      "train loss:0.3026601915546459\n",
      "train loss:0.15148315998709463\n",
      "train loss:0.19009491264059808\n",
      "train loss:0.19747669449314206\n",
      "train loss:0.0792687373258048\n",
      "train loss:0.14618745288133905\n",
      "train loss:0.2626539872816887\n",
      "train loss:0.22929932033081776\n",
      "train loss:0.18883953018790506\n",
      "=== epoch:16, train acc:0.929, test acc:0.866 ===\n",
      "train loss:0.24304829362874159\n",
      "train loss:0.12353325167222107\n",
      "train loss:0.2090904182294288\n",
      "train loss:0.17612572973802074\n",
      "train loss:0.2406993176128375\n",
      "train loss:0.1627725969313969\n",
      "train loss:0.11121390786897008\n",
      "train loss:0.11821430930560327\n",
      "train loss:0.2186205697925457\n",
      "train loss:0.13580966064999675\n",
      "train loss:0.22085284259134944\n",
      "train loss:0.13816474589934025\n",
      "train loss:0.091600732078987\n",
      "train loss:0.1166647098568073\n",
      "train loss:0.09232559160962348\n",
      "train loss:0.16868735636031054\n",
      "train loss:0.216306324816948\n",
      "train loss:0.1969813458238354\n",
      "train loss:0.15334378065149257\n",
      "train loss:0.11619585054195765\n",
      "train loss:0.1085519600433128\n",
      "train loss:0.1961978293743987\n",
      "train loss:0.15954864199661675\n",
      "train loss:0.17509535617794736\n",
      "train loss:0.1781590333749878\n",
      "train loss:0.09163671216742232\n",
      "train loss:0.1691607298244457\n",
      "train loss:0.16166445417177885\n",
      "train loss:0.1737078015365417\n",
      "train loss:0.3225488125586596\n",
      "train loss:0.14871242516990757\n",
      "train loss:0.11261758140671567\n",
      "train loss:0.11985486242682836\n",
      "train loss:0.10578970566181982\n",
      "train loss:0.19598311527635898\n",
      "train loss:0.1617558455177879\n",
      "train loss:0.2636672940136943\n",
      "train loss:0.2195541274857148\n",
      "train loss:0.1745424478103812\n",
      "train loss:0.3245377350005896\n",
      "train loss:0.22975587205258002\n",
      "train loss:0.07082782529089107\n",
      "train loss:0.14842032011235343\n",
      "train loss:0.10401862583957935\n",
      "train loss:0.21731522268940073\n",
      "train loss:0.08483032486958349\n",
      "train loss:0.16196186426196552\n",
      "train loss:0.11103906818273833\n",
      "train loss:0.22578304891353665\n",
      "train loss:0.12382533842597267\n",
      "=== epoch:17, train acc:0.932, test acc:0.862 ===\n",
      "train loss:0.13200099198002252\n",
      "train loss:0.22326225857249082\n",
      "train loss:0.2588508287614022\n",
      "train loss:0.12420750997867397\n",
      "train loss:0.08884823215268357\n",
      "train loss:0.21215484667885182\n",
      "train loss:0.13896631287250183\n",
      "train loss:0.14469590223102646\n",
      "train loss:0.20104096901184693\n",
      "train loss:0.2580120175879643\n",
      "train loss:0.1928861785941421\n",
      "train loss:0.07322707997523882\n",
      "train loss:0.3387423522527526\n",
      "train loss:0.07892895348960362\n",
      "train loss:0.20483822485012376\n",
      "train loss:0.11546964554847376\n",
      "train loss:0.19192098417319195\n",
      "train loss:0.19104182674340245\n",
      "train loss:0.08769935321468283\n",
      "train loss:0.12935646700081813\n",
      "train loss:0.2529896927522625\n",
      "train loss:0.12031598229876367\n",
      "train loss:0.1367612462000878\n",
      "train loss:0.10882386875094982\n",
      "train loss:0.13709211604933613\n",
      "train loss:0.3828082424416309\n",
      "train loss:0.3052825103438696\n",
      "train loss:0.27664036827957744\n",
      "train loss:0.12472831513241361\n",
      "train loss:0.3042432437725664\n",
      "train loss:0.1437895643462982\n",
      "train loss:0.07058816408386176\n",
      "train loss:0.16284651377769727\n",
      "train loss:0.1335625433159338\n",
      "train loss:0.15665480012806143\n",
      "train loss:0.10706818744746173\n",
      "train loss:0.18771056005229433\n",
      "train loss:0.2130712949893489\n",
      "train loss:0.16556698258243951\n",
      "train loss:0.13277154381020032\n",
      "train loss:0.07605751977897407\n",
      "train loss:0.1867282961682362\n",
      "train loss:0.10841164516504649\n",
      "train loss:0.11897673069553034\n",
      "train loss:0.0777553503156282\n",
      "train loss:0.1401903058640916\n",
      "train loss:0.25593303633807474\n",
      "train loss:0.14051835057806053\n",
      "train loss:0.09448768407224423\n",
      "train loss:0.1391839804346842\n",
      "=== epoch:18, train acc:0.953, test acc:0.871 ===\n",
      "train loss:0.20514931978025464\n",
      "train loss:0.10718703681790652\n",
      "train loss:0.22989707080917376\n",
      "train loss:0.188503320606689\n",
      "train loss:0.16606487745513931\n",
      "train loss:0.15506706199767314\n",
      "train loss:0.1115319892208527\n",
      "train loss:0.17791809621996893\n",
      "train loss:0.18281382132082336\n",
      "train loss:0.10723033743100735\n",
      "train loss:0.10532217671962318\n",
      "train loss:0.2573374814675925\n",
      "train loss:0.18461784391008645\n",
      "train loss:0.19319239847008496\n",
      "train loss:0.2117890434286832\n",
      "train loss:0.07488536330158237\n",
      "train loss:0.11644952185414736\n",
      "train loss:0.07741223955619607\n",
      "train loss:0.08810865339340467\n",
      "train loss:0.11690923853761027\n",
      "train loss:0.04690375307268121\n",
      "train loss:0.08227070571180038\n",
      "train loss:0.23159420304179046\n",
      "train loss:0.09863247355010833\n",
      "train loss:0.2665067595125801\n",
      "train loss:0.12352902323361631\n",
      "train loss:0.148693342570361\n",
      "train loss:0.14972461679593213\n",
      "train loss:0.13706077737762037\n",
      "train loss:0.15193647159626136\n",
      "train loss:0.1004872565219338\n",
      "train loss:0.1152147790837879\n",
      "train loss:0.09740452225080552\n",
      "train loss:0.1549703598315589\n",
      "train loss:0.1404553115652274\n",
      "train loss:0.15293555333731004\n",
      "train loss:0.2539100117491978\n",
      "train loss:0.10232212512887015\n",
      "train loss:0.10210994635572611\n",
      "train loss:0.06425367572678342\n",
      "train loss:0.12968332858863263\n",
      "train loss:0.14756279999296257\n",
      "train loss:0.07577333628179747\n",
      "train loss:0.16048132208094923\n",
      "train loss:0.07922884446966924\n",
      "train loss:0.07994191311601333\n",
      "train loss:0.08831422557031515\n",
      "train loss:0.09026399390834458\n",
      "train loss:0.10672317274358806\n",
      "train loss:0.08867266556539645\n",
      "=== epoch:19, train acc:0.957, test acc:0.872 ===\n",
      "train loss:0.08469460840493406\n",
      "train loss:0.17512145074269708\n",
      "train loss:0.08081816224071856\n",
      "train loss:0.07603461157480926\n",
      "train loss:0.08146145886073894\n",
      "train loss:0.17610740341092326\n",
      "train loss:0.060807983782866434\n",
      "train loss:0.08209551150175262\n",
      "train loss:0.08954615719808072\n",
      "train loss:0.03749151925873909\n",
      "train loss:0.13085441159252417\n",
      "train loss:0.12655389742335107\n",
      "train loss:0.1318338405136698\n",
      "train loss:0.10019772939818805\n",
      "train loss:0.11373452504438132\n",
      "train loss:0.12253243200117192\n",
      "train loss:0.08621145197239388\n",
      "train loss:0.1063071443707461\n",
      "train loss:0.07033852616780399\n",
      "train loss:0.15768432288570236\n",
      "train loss:0.07773698871119196\n",
      "train loss:0.22654459614868322\n",
      "train loss:0.22727807298752828\n",
      "train loss:0.08634651203695616\n",
      "train loss:0.11310476459368081\n",
      "train loss:0.047495946237356826\n",
      "train loss:0.16687848730270408\n",
      "train loss:0.08655488404207948\n",
      "train loss:0.05031857898751563\n",
      "train loss:0.15412735365767952\n",
      "train loss:0.045098756976357394\n",
      "train loss:0.04931775574962566\n",
      "train loss:0.1785210078267342\n",
      "train loss:0.1069201684312627\n",
      "train loss:0.107494822785466\n",
      "train loss:0.18100220525983193\n",
      "train loss:0.19578801817577535\n",
      "train loss:0.09629041751262551\n",
      "train loss:0.0717140306048008\n",
      "train loss:0.04556459089932411\n",
      "train loss:0.13036933187495084\n",
      "train loss:0.12861709312009462\n",
      "train loss:0.15617315797336415\n",
      "train loss:0.09677530551958434\n",
      "train loss:0.055781614393379045\n",
      "train loss:0.0542272880072815\n",
      "train loss:0.09531946354519923\n",
      "train loss:0.1361806245322411\n",
      "train loss:0.1267193646513386\n",
      "train loss:0.09720865780210247\n",
      "=== epoch:20, train acc:0.962, test acc:0.883 ===\n",
      "train loss:0.10240797962948175\n",
      "train loss:0.15764384572226597\n",
      "train loss:0.13189929388811167\n",
      "train loss:0.0650648620953092\n",
      "train loss:0.05596662813555302\n",
      "train loss:0.09081874106203176\n",
      "train loss:0.0842799783887025\n",
      "train loss:0.10101115719224435\n",
      "train loss:0.048826047127854544\n",
      "train loss:0.08334033024574036\n",
      "train loss:0.09302004441813011\n",
      "train loss:0.04824003464727758\n",
      "train loss:0.0703716877601458\n",
      "train loss:0.08201479264966442\n",
      "train loss:0.16073216839084598\n",
      "train loss:0.05155862597856745\n",
      "train loss:0.06662098273834198\n",
      "train loss:0.07083683072512749\n",
      "train loss:0.11004997303485953\n",
      "train loss:0.1327511186776422\n",
      "train loss:0.08906919881610131\n",
      "train loss:0.10036197330786802\n",
      "train loss:0.046209642084163595\n",
      "train loss:0.09844880646198062\n",
      "train loss:0.086746894236709\n",
      "train loss:0.06602027400007303\n",
      "train loss:0.055875900146034295\n",
      "train loss:0.08869354933653316\n",
      "train loss:0.07725388796592006\n",
      "train loss:0.13178145242596437\n",
      "train loss:0.12585319026278163\n",
      "train loss:0.09298489248699882\n",
      "train loss:0.07968947541618082\n",
      "train loss:0.1372970897253643\n",
      "train loss:0.0433174600380663\n",
      "train loss:0.08956854039592754\n",
      "train loss:0.08586729376887585\n",
      "train loss:0.08640668947702228\n",
      "train loss:0.07358674794871504\n",
      "train loss:0.07668547611907765\n",
      "train loss:0.0769636682378898\n",
      "train loss:0.062091029680022745\n",
      "train loss:0.13576776966422913\n",
      "train loss:0.04542227480355864\n",
      "train loss:0.06260718084109826\n",
      "train loss:0.07312480164274716\n",
      "train loss:0.07313697288945448\n",
      "train loss:0.0722255643108571\n",
      "train loss:0.045110405682698085\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.889\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "mlp = SimpleMLP(input_dim=(1, 28, 28))\n",
    "\n",
    "# 학습 수행\n",
    "trainer = Trainer(mlp, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save_params(\"params_self.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
